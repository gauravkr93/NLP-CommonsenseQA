{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before running this notebook, make sure to upload the `CommonsenseQA` folder as a zipped file to the working directory\n",
    "- `/home/jupyter/` in case of Google AI platform notebooks.\n",
    "- `/content/` in case of Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm GPU\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  CommonsenseQA.zip\n",
      "   creating: CommonsenseQA/\n",
      "   creating: CommonsenseQA/data/\n",
      "   creating: CommonsenseQA/data/CommonsenseQA/\n",
      "  inflating: CommonsenseQA/data/CommonsenseQA/dict.txt  \n",
      "  inflating: CommonsenseQA/data/CommonsenseQA/test.jsonl  \n",
      "  inflating: CommonsenseQA/data/CommonsenseQA/train.jsonl  \n",
      "  inflating: CommonsenseQA/data/CommonsenseQA/valid.jsonl  \n",
      "   creating: CommonsenseQA/fairseq/\n",
      "   creating: CommonsenseQA/fairseq/examples/\n",
      "   creating: CommonsenseQA/fairseq/examples/roberta/\n",
      "   creating: CommonsenseQA/fairseq/examples/roberta/commonsense_qa/\n",
      "  inflating: CommonsenseQA/fairseq/examples/roberta/commonsense_qa/commonsense_qa_task.py  \n",
      "  inflating: CommonsenseQA/fairseq/examples/roberta/commonsense_qa/download_cqa_data.sh  \n",
      "  inflating: CommonsenseQA/fairseq/examples/roberta/commonsense_qa/README.md  \n",
      "  inflating: CommonsenseQA/fairseq/examples/roberta/commonsense_qa/__init__.py  \n",
      "  inflating: CommonsenseQA/finetune.sh  \n"
     ]
    }
   ],
   "source": [
    "# Unzip code\n",
    "!unzip CommonsenseQA.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fairseq\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/bf/de299e082e7af010d35162cb9a185dc6c17db71624590f2f379aeb2519ff/fairseq-0.9.0.tar.gz (306kB)\n",
      "\u001b[K     |████████████████████████████████| 307kB 2.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cffi in /opt/anaconda3/lib/python3.7/site-packages (from fairseq) (1.13.0)\n",
      "Requirement already satisfied: cython in /opt/anaconda3/lib/python3.7/site-packages (from fairseq) (0.29.14)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.7/site-packages (from fairseq) (1.18.1)\n",
      "Requirement already satisfied: regex in /opt/anaconda3/lib/python3.7/site-packages (from fairseq) (2019.8.19)\n",
      "Collecting sacrebleu\n",
      "  Downloading https://files.pythonhosted.org/packages/31/67/a895f0aa46891a6af4bd00e98009eae59c065f2b220de663ca3e948da453/sacrebleu-1.4.4-py3-none-any.whl\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.7/site-packages (from fairseq) (1.4.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.7/site-packages (from fairseq) (4.42.0)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.7/site-packages (from cffi->fairseq) (2.19)\n",
      "Collecting portalocker\n",
      "  Downloading https://files.pythonhosted.org/packages/91/db/7bc703c0760df726839e0699b7f78a4d8217fdc9c7fcb1b51b39c5a22a4e/portalocker-1.5.2-py2.py3-none-any.whl\n",
      "Collecting typing\n",
      "  Downloading https://files.pythonhosted.org/packages/fe/2e/b480ee1b75e6d17d2993738670e75c1feeb9ff7f64452153cf018051cc92/typing-3.7.4.1-py3-none-any.whl\n",
      "Building wheels for collected packages: fairseq\n",
      "  Building wheel for fairseq (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fairseq: filename=fairseq-0.9.0-cp37-cp37m-linux_x86_64.whl size=2017047 sha256=ecbc9265858f5e1fac1998b530686de53155ea9f309acd1a01a67641fea74939\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/37/3e/1b/0fa30695dcba41e4b0088067fa40f3328d1e8ee78c22cd4766\n",
      "Successfully built fairseq\n",
      "Installing collected packages: portalocker, typing, sacrebleu, fairseq\n",
      "Successfully installed fairseq-0.9.0 portalocker-1.5.2 sacrebleu-1.4.4 typing-3.7.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-03-13 06:14:29--  https://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.20.22.166, 104.20.6.166, 2606:4700:10::6814:6a6, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.20.22.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 655283069 (625M) [application/gzip]\n",
      "Saving to: ‘roberta.large.tar.gz’\n",
      "\n",
      "roberta.large.tar.g 100%[===================>] 624.93M  31.1MB/s    in 22s     \n",
      "\n",
      "2020-03-13 06:14:51 (29.0 MB/s) - ‘roberta.large.tar.gz’ saved [655283069/655283069]\n",
      "\n",
      "roberta.large/\n",
      "roberta.large/dict.txt\n",
      "roberta.large/model.pt\n",
      "roberta.large/NOTE\n"
     ]
    }
   ],
   "source": [
    "# Download roberta model\n",
    "!wget -O roberta.large.tar.gz https://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz\n",
    "!tar -xvzf roberta.large.tar.gz\n",
    "\n",
    "# !wget -O /content/CommonsenseQA/roberta.base.tar.gz https://dl.fbaipublicfiles.com/fairseq/models/roberta.base.tar.gz\n",
    "# !tar -xvzf /content/CommonsenseQA/roberta.base.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/CommonsenseQA\n",
      "/home/jupyter/CommonsenseQA\n"
     ]
    }
   ],
   "source": [
    "%cd /home/jupyter/CommonsenseQA\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting finetune.sh\n"
     ]
    }
   ],
   "source": [
    "## Write the finetuning part to a bash script file\n",
    "# Modified following from the original script to get it to run on Google AI platform and Colab\n",
    "# - Set MAX_SENTENCES=8\n",
    "# - Added --update-freq 4\n",
    "\n",
    "%%writefile finetune.sh\n",
    "#!/bin/bash\n",
    "\n",
    "MAX_UPDATES=3000      # Number of training steps.\n",
    "WARMUP_UPDATES=150    # Linearly increase LR over this many steps.\n",
    "LR=1e-05              # Peak LR for polynomial LR scheduler.\n",
    "MAX_SENTENCES=8      # Batch size.\n",
    "SEED=23                # Random seed.\n",
    "\n",
    "BASEDIR=/home/jupyter\n",
    "# CQA_PATH=/content/CommonsenseQA # For Google Colab\n",
    "CQA_PATH=$BASEDIR/CommonsenseQA # For Kaggle\n",
    "ROBERTA_PATH=${BASEDIR}/roberta.large/model.pt\n",
    "DATA_DIR=${CQA_PATH}/data/CommonsenseQA\n",
    "\n",
    "# we use the --user-dir option to load the task from\n",
    "# the examples/roberta/commonsense_qa directory:\n",
    "FAIRSEQ_PATH=${CQA_PATH}/fairseq\n",
    "FAIRSEQ_USER_DIR=${FAIRSEQ_PATH}/examples/roberta/commonsense_qa\n",
    "\n",
    "cd $FAIRSEQ_PATH\n",
    "CUDA_VISIBLE_DEVICES=0 fairseq-train --fp16 --ddp-backend=no_c10d \\\n",
    "    $DATA_DIR \\\n",
    "    --update-freq 4 \\\n",
    "    --save-dir ./checkpoints \\\n",
    "    --user-dir $FAIRSEQ_USER_DIR \\\n",
    "    --restore-file $ROBERTA_PATH \\\n",
    "    --reset-optimizer --reset-dataloader --reset-meters \\\n",
    "    --no-epoch-checkpoints --no-last-checkpoints --no-save-optimizer-state \\\n",
    "    --best-checkpoint-metric accuracy --maximize-best-checkpoint-metric \\\n",
    "    --task commonsense_qa --init-token 0 --bpe gpt2 \\\n",
    "    --arch roberta_large --max-positions 512 \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --criterion sentence_ranking --num-classes 5 \\\n",
    "    --optimizer adam --adam-betas '(0.9, 0.98)' --adam-eps 1e-06 --clip-norm 0.0 \\\n",
    "    --lr-scheduler polynomial_decay --lr $LR \\\n",
    "    --warmup-updates $WARMUP_UPDATES --total-num-update $MAX_UPDATES \\\n",
    "    --max-sentences $MAX_SENTENCES \\\n",
    "    --max-update $MAX_UPDATES \\\n",
    "    --log-format simple --log-interval 25 \\\n",
    "    --seed $SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, arch='roberta_large', attention_dropout=0.1, best_checkpoint_metric='accuracy', bpe='gpt2', bucket_cap_mb=25, clip_norm=0.0, cpu=False, criterion='sentence_ranking', curriculum=0, data='/home/jupyter/CommonsenseQA/data/CommonsenseQA', dataset_impl=None, ddp_backend='no_c10d', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=24, encoder_layers_to_keep=None, end_learning_rate=0.0, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gpt2_encoder_json='https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', gpt2_vocab_bpe='https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe', init_token=0, keep_interval_updates=-1, keep_last_epochs=-1, log_format='simple', log_interval=25, lr=[1e-05], lr_scheduler='polynomial_decay', max_epoch=0, max_positions=512, max_sentences=8, max_sentences_valid=8, max_tokens=None, max_tokens_valid=None, max_update=3000, maximize_best_checkpoint_metric=True, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, no_epoch_checkpoints=True, no_last_checkpoints=True, no_progress_bar=False, no_save=False, no_save_optimizer_state=True, num_classes=5, num_workers=1, optimizer='adam', optimizer_overrides='{}', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, required_batch_size_multiple=8, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/jupyter/roberta.large/model.pt', save_dir='./checkpoints', save_interval=1, save_interval_updates=0, save_predictions=None, seed=23, sentence_avg=False, skip_invalid_size_inputs_valid_test=False, task='commonsense_qa', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, total_num_update=3000, train_subset='train', update_freq=[4], use_bmuf=False, user_dir='/home/jupyter/CommonsenseQA/fairseq/examples/roberta/commonsense_qa', valid_subset='valid', validate_interval=1, warmup_updates=150, weight_decay=0.01)\n",
      "| dictionary: 50265 types\n",
      "| Loaded valid with 1221 samples\n",
      "RobertaModel(\n",
      "  (decoder): RobertaEncoder(\n",
      "    (sentence_encoder): TransformerSentenceEncoder(\n",
      "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
      "      (embed_positions): LearnedPositionalEmbedding(514, 1024, padding_idx=1)\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (6): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (8): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (9): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (10): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (11): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (12): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (13): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (14): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (15): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (16): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (17): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (18): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (19): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (20): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (21): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (22): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (23): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (emb_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (lm_head): RobertaLMHead(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (classification_heads): ModuleDict(\n",
      "    (sentence_classification_head): RobertaClassificationHead(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (out_proj): Linear(in_features=1024, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "| model roberta_large, criterion SentenceRankingCriterion\n",
      "| num. model params: 356461658 (num. trained: 356461658)\n",
      "| training on 1 GPUs\n",
      "| max tokens per GPU = None and max sentences per GPU = 8\n",
      "Overwriting classification_heads.sentence_classification_head.dense.weight\n",
      "Overwriting classification_heads.sentence_classification_head.dense.bias\n",
      "Overwriting classification_heads.sentence_classification_head.out_proj.weight\n",
      "Overwriting classification_heads.sentence_classification_head.out_proj.bias\n",
      "| loaded checkpoint /home/jupyter/roberta.large/model.pt (epoch 0 @ 0 updates)\n",
      "| loading train data for epoch 0\n",
      "| Loaded train with 9741 samples\n",
      "| epoch 001:     25 / 305 loss=2.322, nll_loss=0.096, ppl=1.07, wps=497, ups=1, wpb=775.115, bsz=32.000, num_updates=26, lr=1.73333e-06, gnorm=3.140, clip=0.000, oom=0.000, loss_scale=128.000, wall=62, train_wall=41, accuracy=0.217548\n",
      "| epoch 001:     50 / 305 loss=2.322, nll_loss=0.097, ppl=1.07, wps=489, ups=1, wpb=768.333, bsz=32.000, num_updates=51, lr=3.4e-06, gnorm=4.596, clip=0.000, oom=0.000, loss_scale=128.000, wall=102, train_wall=80, accuracy=0.235294\n",
      "| epoch 001:     75 / 305 loss=2.319, nll_loss=0.096, ppl=1.07, wps=491, ups=1, wpb=770.066, bsz=32.000, num_updates=76, lr=5.06667e-06, gnorm=6.079, clip=0.000, oom=0.000, loss_scale=128.000, wall=141, train_wall=119, accuracy=0.244655\n",
      "| epoch 001:    100 / 305 loss=2.315, nll_loss=0.096, ppl=1.07, wps=489, ups=1, wpb=769.713, bsz=32.000, num_updates=101, lr=6.73333e-06, gnorm=8.267, clip=0.000, oom=0.000, loss_scale=128.000, wall=180, train_wall=158, accuracy=0.252475\n",
      "| epoch 001:    125 / 305 loss=2.297, nll_loss=0.095, ppl=1.07, wps=488, ups=1, wpb=770.413, bsz=32.000, num_updates=126, lr=8.4e-06, gnorm=14.985, clip=0.000, oom=0.000, loss_scale=128.000, wall=220, train_wall=197, accuracy=0.263641\n",
      "| WARNING: overflow detected, setting loss scale to: 64.0\n",
      "| epoch 001:    150 / 305 loss=2.264, nll_loss=0.094, ppl=1.07, wps=485, ups=1, wpb=771.707, bsz=32.000, num_updates=150, lr=1e-05, gnorm=21.471, clip=0.000, oom=0.000, loss_scale=64.000, wall=260, train_wall=236, accuracy=0.280208\n",
      "| WARNING: overflow detected, setting loss scale to: 32.0\n",
      "| epoch 001:    175 / 305 loss=2.212, nll_loss=0.092, ppl=1.07, wps=481, ups=1, wpb=771.155, bsz=31.983, num_updates=174, lr=9.91579e-06, gnorm=28.260, clip=0.000, oom=0.000, loss_scale=32.000, wall=300, train_wall=276, accuracy=0.309434\n",
      "| epoch 001:    200 / 305 loss=2.157, nll_loss=0.089, ppl=1.06, wps=481, ups=1, wpb=771.347, bsz=31.985, num_updates=199, lr=9.82807e-06, gnorm=32.936, clip=0.000, oom=0.000, loss_scale=32.000, wall=340, train_wall=315, accuracy=0.3315\n",
      "| epoch 001:    225 / 305 loss=2.108, nll_loss=0.087, ppl=1.06, wps=481, ups=1, wpb=771.839, bsz=31.987, num_updates=224, lr=9.74035e-06, gnorm=34.166, clip=0.000, oom=0.000, loss_scale=32.000, wall=381, train_wall=355, accuracy=0.352966\n",
      "| epoch 001:    250 / 305 loss=2.051, nll_loss=0.085, ppl=1.06, wps=481, ups=1, wpb=772.221, bsz=31.988, num_updates=249, lr=9.65263e-06, gnorm=34.524, clip=0.000, oom=0.000, loss_scale=32.000, wall=421, train_wall=395, accuracy=0.375267\n",
      "| epoch 001:    275 / 305 loss=2.001, nll_loss=0.083, ppl=1.06, wps=480, ups=1, wpb=771.956, bsz=31.989, num_updates=274, lr=9.56491e-06, gnorm=34.217, clip=0.000, oom=0.000, loss_scale=32.000, wall=462, train_wall=434, accuracy=0.395322\n",
      "| epoch 001:    300 / 305 loss=1.955, nll_loss=0.081, ppl=1.06, wps=480, ups=1, wpb=772.164, bsz=31.990, num_updates=299, lr=9.47719e-06, gnorm=34.207, clip=0.000, oom=0.000, loss_scale=32.000, wall=502, train_wall=474, accuracy=0.412128\n",
      "| epoch 001 | loss 1.947 | nll_loss 0.081 | ppl 1.06 | wps 480 | ups 1 | wpb 770.574 | bsz 31.937 | num_updates 303 | lr 9.46316e-06 | gnorm 34.197 | clip 0.000 | oom 0.000 | loss_scale 32.000 | wall 508 | train_wall 480 | accuracy 0.414695\n",
      "| epoch 001 | valid on 'valid' subset | loss 1.094 | nll_loss 0.046 | ppl 1.03 | num_updates 303 | accuracy 0.733007\n",
      "| saved checkpoint ./checkpoints/checkpoint_best.pt (epoch 1 @ 303 updates) (writing took 5.9812281131744385 seconds)\n",
      "| epoch 002:     25 / 305 loss=1.292, nll_loss=0.054, ppl=1.04, wps=474, ups=1, wpb=767.038, bsz=32.000, num_updates=329, lr=9.37193e-06, gnorm=42.538, clip=0.000, oom=0.000, loss_scale=32.000, wall=574, train_wall=521, accuracy=0.628606\n",
      "| epoch 002:     50 / 305 loss=1.325, nll_loss=0.055, ppl=1.04, wps=478, ups=1, wpb=769.471, bsz=32.000, num_updates=354, lr=9.28421e-06, gnorm=41.430, clip=0.000, oom=0.000, loss_scale=32.000, wall=614, train_wall=560, accuracy=0.620711\n",
      "| epoch 002:     75 / 305 loss=1.308, nll_loss=0.055, ppl=1.04, wps=477, ups=1, wpb=766.053, bsz=32.000, num_updates=379, lr=9.19649e-06, gnorm=39.183, clip=0.000, oom=0.000, loss_scale=32.000, wall=654, train_wall=600, accuracy=0.622944\n",
      "| epoch 002:    100 / 305 loss=1.293, nll_loss=0.054, ppl=1.04, wps=479, ups=1, wpb=770.713, bsz=32.000, num_updates=404, lr=9.10877e-06, gnorm=36.853, clip=0.000, oom=0.000, loss_scale=32.000, wall=695, train_wall=639, accuracy=0.634592\n",
      "| epoch 002:    125 / 305 loss=1.294, nll_loss=0.054, ppl=1.04, wps=479, ups=1, wpb=769.238, bsz=32.000, num_updates=429, lr=9.02105e-06, gnorm=36.025, clip=0.000, oom=0.000, loss_scale=32.000, wall=735, train_wall=679, accuracy=0.636161\n",
      "| epoch 002:    150 / 305 loss=1.281, nll_loss=0.053, ppl=1.04, wps=479, ups=1, wpb=769.152, bsz=32.000, num_updates=454, lr=8.93333e-06, gnorm=35.252, clip=0.000, oom=0.000, loss_scale=32.000, wall=775, train_wall=718, accuracy=0.64197\n",
      "| epoch 002:    175 / 305 loss=1.272, nll_loss=0.053, ppl=1.04, wps=480, ups=1, wpb=770.636, bsz=32.000, num_updates=479, lr=8.84561e-06, gnorm=34.465, clip=0.000, oom=0.000, loss_scale=32.000, wall=815, train_wall=758, accuracy=0.645241\n",
      "| epoch 002:    200 / 305 loss=1.269, nll_loss=0.053, ppl=1.04, wps=481, ups=1, wpb=772.234, bsz=32.000, num_updates=504, lr=8.75789e-06, gnorm=34.070, clip=0.000, oom=0.000, loss_scale=32.000, wall=855, train_wall=797, accuracy=0.645211\n",
      "| epoch 002:    225 / 305 loss=1.259, nll_loss=0.052, ppl=1.04, wps=480, ups=1, wpb=771.155, bsz=31.987, num_updates=529, lr=8.67018e-06, gnorm=34.250, clip=0.000, oom=0.000, loss_scale=32.000, wall=895, train_wall=837, accuracy=0.650436\n",
      "| epoch 002:    250 / 305 loss=1.259, nll_loss=0.052, ppl=1.04, wps=481, ups=1, wpb=772.351, bsz=31.988, num_updates=554, lr=8.58246e-06, gnorm=34.488, clip=0.000, oom=0.000, loss_scale=32.000, wall=935, train_wall=876, accuracy=0.652759\n",
      "| epoch 002:    275 / 305 loss=1.254, nll_loss=0.052, ppl=1.04, wps=482, ups=1, wpb=771.808, bsz=31.989, num_updates=579, lr=8.49474e-06, gnorm=35.041, clip=0.000, oom=0.000, loss_scale=32.000, wall=975, train_wall=915, accuracy=0.65568\n",
      "| epoch 002:    300 / 305 loss=1.245, nll_loss=0.052, ppl=1.04, wps=481, ups=1, wpb=771.668, bsz=31.990, num_updates=604, lr=8.40702e-06, gnorm=37.748, clip=0.000, oom=0.000, loss_scale=32.000, wall=1015, train_wall=954, accuracy=0.660297\n",
      "| epoch 002 | loss 1.244 | nll_loss 0.052 | ppl 1.04 | wps 481 | ups 1 | wpb 770.593 | bsz 31.938 | num_updates 608 | lr 8.39298e-06 | gnorm 37.876 | clip 0.000 | oom 0.000 | loss_scale 32.000 | wall 1021 | train_wall 960 | accuracy 0.660507\n",
      "| epoch 002 | valid on 'valid' subset | loss 0.935 | nll_loss 0.039 | ppl 1.03 | num_updates 608 | best_accuracy 0.746895 | accuracy 0.746895\n",
      "| saved checkpoint ./checkpoints/checkpoint_best.pt (epoch 2 @ 608 updates) (writing took 5.617724418640137 seconds)\n",
      "| epoch 003:     25 / 305 loss=0.920, nll_loss=0.038, ppl=1.03, wps=489, ups=1, wpb=776.192, bsz=32.000, num_updates=634, lr=8.30175e-06, gnorm=54.722, clip=0.000, oom=0.000, loss_scale=32.000, wall=1086, train_wall=1001, accuracy=0.772837\n",
      "| epoch 003:     50 / 305 loss=0.949, nll_loss=0.039, ppl=1.03, wps=487, ups=1, wpb=773.294, bsz=31.941, num_updates=659, lr=8.21404e-06, gnorm=49.232, clip=0.000, oom=0.000, loss_scale=32.000, wall=1125, train_wall=1040, accuracy=0.750153\n",
      "| epoch 003:     75 / 305 loss=0.922, nll_loss=0.038, ppl=1.03, wps=486, ups=1, wpb=772.237, bsz=31.961, num_updates=684, lr=8.12632e-06, gnorm=44.944, clip=0.000, oom=0.000, loss_scale=32.000, wall=1165, train_wall=1079, accuracy=0.752573\n",
      "| epoch 003:    100 / 305 loss=0.920, nll_loss=0.038, ppl=1.03, wps=486, ups=1, wpb=774.446, bsz=31.970, num_updates=709, lr=8.0386e-06, gnorm=43.775, clip=0.000, oom=0.000, loss_scale=32.000, wall=1205, train_wall=1118, accuracy=0.755962\n",
      "| epoch 003:    125 / 305 loss=0.914, nll_loss=0.038, ppl=1.03, wps=487, ups=1, wpb=775.698, bsz=31.976, num_updates=734, lr=7.95088e-06, gnorm=42.122, clip=0.000, oom=0.000, loss_scale=32.000, wall=1245, train_wall=1158, accuracy=0.756019\n",
      "| epoch 003:    150 / 305 loss=0.905, nll_loss=0.037, ppl=1.03, wps=486, ups=1, wpb=773.987, bsz=31.980, num_updates=759, lr=7.86316e-06, gnorm=41.951, clip=0.000, oom=0.000, loss_scale=32.000, wall=1285, train_wall=1197, accuracy=0.756471\n",
      "| epoch 003:    175 / 305 loss=0.908, nll_loss=0.038, ppl=1.03, wps=486, ups=1, wpb=773.159, bsz=31.983, num_updates=784, lr=7.77544e-06, gnorm=41.078, clip=0.000, oom=0.000, loss_scale=32.000, wall=1325, train_wall=1236, accuracy=0.757506\n",
      "| epoch 003:    200 / 305 loss=0.907, nll_loss=0.038, ppl=1.03, wps=486, ups=1, wpb=772.667, bsz=31.985, num_updates=809, lr=7.68772e-06, gnorm=40.585, clip=0.000, oom=0.000, loss_scale=32.000, wall=1364, train_wall=1275, accuracy=0.75595\n",
      "| epoch 003:    225 / 305 loss=0.916, nll_loss=0.038, ppl=1.03, wps=486, ups=1, wpb=772.854, bsz=31.987, num_updates=834, lr=7.6e-06, gnorm=40.291, clip=0.000, oom=0.000, loss_scale=32.000, wall=1404, train_wall=1314, accuracy=0.7546\n",
      "| epoch 003:    250 / 305 loss=0.908, nll_loss=0.038, ppl=1.03, wps=486, ups=1, wpb=773.773, bsz=31.988, num_updates=859, lr=7.51228e-06, gnorm=40.258, clip=0.000, oom=0.000, loss_scale=32.000, wall=1444, train_wall=1353, accuracy=0.756383\n",
      "| epoch 003:    275 / 305 loss=0.911, nll_loss=0.038, ppl=1.03, wps=486, ups=1, wpb=772.895, bsz=31.989, num_updates=884, lr=7.42456e-06, gnorm=39.608, clip=0.000, oom=0.000, loss_scale=32.000, wall=1483, train_wall=1392, accuracy=0.755012\n",
      "| epoch 003:    300 / 305 loss=0.911, nll_loss=0.038, ppl=1.03, wps=486, ups=1, wpb=772.173, bsz=31.990, num_updates=909, lr=7.33684e-06, gnorm=41.294, clip=0.000, oom=0.000, loss_scale=32.000, wall=1522, train_wall=1430, accuracy=0.753765\n",
      "| epoch 003 | loss 0.912 | nll_loss 0.038 | ppl 1.03 | wps 486 | ups 1 | wpb 770.593 | bsz 31.938 | num_updates 913 | lr 7.32281e-06 | gnorm 41.252 | clip 0.000 | oom 0.000 | loss_scale 32.000 | wall 1528 | train_wall 1436 | accuracy 0.753619\n",
      "| epoch 003 | valid on 'valid' subset | loss 0.899 | nll_loss 0.038 | ppl 1.03 | num_updates 913 | best_accuracy 0.768137 | accuracy 0.768137\n",
      "| saved checkpoint ./checkpoints/checkpoint_best.pt (epoch 3 @ 913 updates) (writing took 4.929204702377319 seconds)\n",
      "| WARNING: overflow detected, setting loss scale to: 16.0\n",
      "| epoch 004:     25 / 305 loss=0.656, nll_loss=0.028, ppl=1.02, wps=464, ups=1, wpb=762.600, bsz=32.000, num_updates=938, lr=7.23509e-06, gnorm=44.815, clip=0.000, oom=0.000, loss_scale=16.000, wall=1592, train_wall=1476, accuracy=0.8275\n",
      "| epoch 004:     50 / 305 loss=0.671, nll_loss=0.028, ppl=1.02, wps=472, ups=1, wpb=760.840, bsz=32.000, num_updates=963, lr=7.14737e-06, gnorm=50.075, clip=0.000, oom=0.000, loss_scale=16.000, wall=1632, train_wall=1515, accuracy=0.823125\n",
      "| epoch 004:     75 / 305 loss=0.670, nll_loss=0.028, ppl=1.02, wps=479, ups=1, wpb=768.333, bsz=32.000, num_updates=988, lr=7.05965e-06, gnorm=48.850, clip=0.000, oom=0.000, loss_scale=16.000, wall=1671, train_wall=1554, accuracy=0.821667\n",
      "| epoch 004:    100 / 305 loss=0.670, nll_loss=0.028, ppl=1.02, wps=481, ups=1, wpb=771.120, bsz=32.000, num_updates=1013, lr=6.97193e-06, gnorm=45.660, clip=0.000, oom=0.000, loss_scale=16.000, wall=1711, train_wall=1594, accuracy=0.822812\n",
      "| epoch 004:    125 / 305 loss=0.658, nll_loss=0.027, ppl=1.02, wps=481, ups=1, wpb=769.960, bsz=32.000, num_updates=1038, lr=6.88421e-06, gnorm=44.363, clip=0.000, oom=0.000, loss_scale=16.000, wall=1751, train_wall=1633, accuracy=0.82675\n",
      "| epoch 004:    150 / 305 loss=0.648, nll_loss=0.027, ppl=1.02, wps=482, ups=1, wpb=769.387, bsz=32.000, num_updates=1063, lr=6.79649e-06, gnorm=44.678, clip=0.000, oom=0.000, loss_scale=16.000, wall=1791, train_wall=1672, accuracy=0.829375\n",
      "| epoch 004:    175 / 305 loss=0.657, nll_loss=0.027, ppl=1.02, wps=483, ups=1, wpb=769.863, bsz=32.000, num_updates=1088, lr=6.70877e-06, gnorm=47.005, clip=0.000, oom=0.000, loss_scale=16.000, wall=1830, train_wall=1710, accuracy=0.826964\n",
      "| epoch 004:    200 / 305 loss=0.654, nll_loss=0.027, ppl=1.02, wps=484, ups=1, wpb=769.405, bsz=32.000, num_updates=1113, lr=6.62105e-06, gnorm=45.882, clip=0.000, oom=0.000, loss_scale=16.000, wall=1869, train_wall=1749, accuracy=0.828281\n",
      "| epoch 004:    225 / 305 loss=0.661, nll_loss=0.027, ppl=1.02, wps=485, ups=1, wpb=770.004, bsz=32.000, num_updates=1138, lr=6.53333e-06, gnorm=44.409, clip=0.000, oom=0.000, loss_scale=16.000, wall=1909, train_wall=1788, accuracy=0.825972\n",
      "| epoch 004:    250 / 305 loss=0.669, nll_loss=0.028, ppl=1.02, wps=484, ups=1, wpb=770.604, bsz=32.000, num_updates=1163, lr=6.44561e-06, gnorm=43.761, clip=0.000, oom=0.000, loss_scale=16.000, wall=1949, train_wall=1828, accuracy=0.823125\n",
      "| epoch 004:    275 / 305 loss=0.675, nll_loss=0.028, ppl=1.02, wps=485, ups=1, wpb=771.873, bsz=32.000, num_updates=1188, lr=6.35789e-06, gnorm=42.741, clip=0.000, oom=0.000, loss_scale=16.000, wall=1989, train_wall=1867, accuracy=0.821477\n",
      "| epoch 004:    300 / 305 loss=0.673, nll_loss=0.028, ppl=1.02, wps=484, ups=1, wpb=771.887, bsz=31.990, num_updates=1213, lr=6.27018e-06, gnorm=42.051, clip=0.000, oom=0.000, loss_scale=16.000, wall=2029, train_wall=1907, accuracy=0.821194\n",
      "| epoch 004 | loss 0.672 | nll_loss 0.028 | ppl 1.02 | wps 484 | ups 1 | wpb 770.592 | bsz 31.938 | num_updates 1217 | lr 6.25614e-06 | gnorm 42.047 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 2035 | train_wall 1912 | accuracy 0.8213\n",
      "| epoch 004 | valid on 'valid' subset | loss 0.962 | nll_loss 0.040 | ppl 1.03 | num_updates 1217 | best_accuracy 0.779575 | accuracy 0.779575\n",
      "| saved checkpoint ./checkpoints/checkpoint_best.pt (epoch 4 @ 1217 updates) (writing took 5.669405937194824 seconds)\n",
      "| epoch 005:     25 / 305 loss=0.484, nll_loss=0.020, ppl=1.01, wps=480, ups=1, wpb=764.154, bsz=32.000, num_updates=1243, lr=6.16491e-06, gnorm=31.952, clip=0.000, oom=0.000, loss_scale=16.000, wall=2100, train_wall=1953, accuracy=0.878606\n",
      "| epoch 005:     50 / 305 loss=0.468, nll_loss=0.019, ppl=1.01, wps=482, ups=1, wpb=774.098, bsz=32.000, num_updates=1268, lr=6.07719e-06, gnorm=33.486, clip=0.000, oom=0.000, loss_scale=16.000, wall=2141, train_wall=1993, accuracy=0.877451\n",
      "| epoch 005:     75 / 305 loss=0.472, nll_loss=0.019, ppl=1.01, wps=485, ups=1, wpb=776.197, bsz=32.000, num_updates=1293, lr=5.98947e-06, gnorm=34.750, clip=0.000, oom=0.000, loss_scale=16.000, wall=2181, train_wall=2032, accuracy=0.877878\n",
      "| epoch 005:    100 / 305 loss=0.500, nll_loss=0.021, ppl=1.01, wps=487, ups=1, wpb=777.248, bsz=32.000, num_updates=1318, lr=5.90175e-06, gnorm=35.419, clip=0.000, oom=0.000, loss_scale=16.000, wall=2220, train_wall=2071, accuracy=0.870359\n",
      "| epoch 005:    125 / 305 loss=0.500, nll_loss=0.021, ppl=1.01, wps=486, ups=1, wpb=776.937, bsz=32.000, num_updates=1343, lr=5.81404e-06, gnorm=37.418, clip=0.000, oom=0.000, loss_scale=16.000, wall=2260, train_wall=2110, accuracy=0.871776\n",
      "| epoch 005:    150 / 305 loss=0.505, nll_loss=0.021, ppl=1.01, wps=487, ups=1, wpb=775.503, bsz=32.000, num_updates=1368, lr=5.72632e-06, gnorm=37.524, clip=0.000, oom=0.000, loss_scale=16.000, wall=2300, train_wall=2149, accuracy=0.87024\n",
      "| epoch 005:    175 / 305 loss=0.495, nll_loss=0.020, ppl=1.01, wps=487, ups=1, wpb=775.455, bsz=32.000, num_updates=1393, lr=5.6386e-06, gnorm=36.329, clip=0.000, oom=0.000, loss_scale=16.000, wall=2339, train_wall=2188, accuracy=0.872337\n",
      "| epoch 005:    200 / 305 loss=0.491, nll_loss=0.020, ppl=1.01, wps=487, ups=1, wpb=774.244, bsz=31.985, num_updates=1418, lr=5.55088e-06, gnorm=36.360, clip=0.000, oom=0.000, loss_scale=16.000, wall=2379, train_wall=2227, accuracy=0.872297\n",
      "| epoch 005:    225 / 305 loss=0.496, nll_loss=0.021, ppl=1.01, wps=487, ups=1, wpb=773.588, bsz=31.987, num_updates=1443, lr=5.46316e-06, gnorm=36.169, clip=0.000, oom=0.000, loss_scale=16.000, wall=2418, train_wall=2265, accuracy=0.87066\n",
      "| epoch 005:    250 / 305 loss=0.498, nll_loss=0.021, ppl=1.01, wps=487, ups=1, wpb=773.303, bsz=31.988, num_updates=1468, lr=5.37544e-06, gnorm=36.936, clip=0.000, oom=0.000, loss_scale=16.000, wall=2458, train_wall=2305, accuracy=0.869971\n",
      "| epoch 005:    275 / 305 loss=0.499, nll_loss=0.021, ppl=1.01, wps=486, ups=1, wpb=772.641, bsz=31.989, num_updates=1493, lr=5.28772e-06, gnorm=36.886, clip=0.000, oom=0.000, loss_scale=16.000, wall=2497, train_wall=2344, accuracy=0.869294\n",
      "| epoch 005:    300 / 305 loss=0.503, nll_loss=0.021, ppl=1.01, wps=486, ups=1, wpb=771.488, bsz=31.990, num_updates=1518, lr=5.2e-06, gnorm=36.777, clip=0.000, oom=0.000, loss_scale=16.000, wall=2537, train_wall=2383, accuracy=0.867276\n",
      "| epoch 005 | loss 0.501 | nll_loss 0.021 | ppl 1.01 | wps 486 | ups 1 | wpb 770.593 | bsz 31.938 | num_updates 1522 | lr 5.18596e-06 | gnorm 36.673 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 2543 | train_wall 2388 | accuracy 0.867673\n",
      "| epoch 005 | valid on 'valid' subset | loss 1.060 | nll_loss 0.044 | ppl 1.03 | num_updates 1522 | best_accuracy 0.779575 | accuracy 0.768137\n",
      "| epoch 006:     25 / 305 loss=0.337, nll_loss=0.014, ppl=1.01, wps=487, ups=1, wpb=775.962, bsz=32.000, num_updates=1548, lr=5.09474e-06, gnorm=28.036, clip=0.000, oom=0.000, loss_scale=16.000, wall=2602, train_wall=2429, accuracy=0.917067\n",
      "| epoch 006:     50 / 305 loss=0.341, nll_loss=0.014, ppl=1.01, wps=487, ups=1, wpb=773.333, bsz=32.000, num_updates=1573, lr=5.00702e-06, gnorm=29.563, clip=0.000, oom=0.000, loss_scale=16.000, wall=2641, train_wall=2468, accuracy=0.912377\n",
      "| epoch 006:     75 / 305 loss=0.366, nll_loss=0.015, ppl=1.01, wps=488, ups=1, wpb=772.750, bsz=32.000, num_updates=1598, lr=4.9193e-06, gnorm=30.312, clip=0.000, oom=0.000, loss_scale=16.000, wall=2681, train_wall=2506, accuracy=0.901727\n",
      "| epoch 006:    100 / 305 loss=0.361, nll_loss=0.015, ppl=1.01, wps=487, ups=1, wpb=771.178, bsz=32.000, num_updates=1623, lr=4.83158e-06, gnorm=29.965, clip=0.000, oom=0.000, loss_scale=16.000, wall=2720, train_wall=2545, accuracy=0.903156\n",
      "| epoch 006:    125 / 305 loss=0.374, nll_loss=0.016, ppl=1.01, wps=486, ups=1, wpb=770.659, bsz=32.000, num_updates=1648, lr=4.74386e-06, gnorm=31.101, clip=0.000, oom=0.000, loss_scale=16.000, wall=2760, train_wall=2585, accuracy=0.900794\n",
      "| epoch 006:    150 / 305 loss=0.377, nll_loss=0.016, ppl=1.01, wps=487, ups=1, wpb=769.748, bsz=31.980, num_updates=1673, lr=4.65614e-06, gnorm=31.000, clip=0.000, oom=0.000, loss_scale=16.000, wall=2799, train_wall=2623, accuracy=0.898116\n",
      "| epoch 006:    175 / 305 loss=0.369, nll_loss=0.015, ppl=1.01, wps=488, ups=1, wpb=771.665, bsz=31.983, num_updates=1698, lr=4.56842e-06, gnorm=30.836, clip=0.000, oom=0.000, loss_scale=16.000, wall=2839, train_wall=2662, accuracy=0.899272\n",
      "| epoch 006:    200 / 305 loss=0.366, nll_loss=0.015, ppl=1.01, wps=486, ups=1, wpb=770.254, bsz=31.985, num_updates=1723, lr=4.4807e-06, gnorm=31.350, clip=0.000, oom=0.000, loss_scale=16.000, wall=2879, train_wall=2701, accuracy=0.899051\n",
      "| epoch 006:    225 / 305 loss=0.360, nll_loss=0.015, ppl=1.01, wps=487, ups=1, wpb=770.363, bsz=31.987, num_updates=1748, lr=4.39298e-06, gnorm=31.596, clip=0.000, oom=0.000, loss_scale=16.000, wall=2918, train_wall=2740, accuracy=0.901923\n",
      "| epoch 006:    250 / 305 loss=0.364, nll_loss=0.015, ppl=1.01, wps=487, ups=1, wpb=771.705, bsz=31.988, num_updates=1773, lr=4.30526e-06, gnorm=31.979, clip=0.000, oom=0.000, loss_scale=16.000, wall=2958, train_wall=2779, accuracy=0.900361\n",
      "| epoch 006:    275 / 305 loss=0.368, nll_loss=0.015, ppl=1.01, wps=488, ups=1, wpb=772.120, bsz=31.989, num_updates=1798, lr=4.21754e-06, gnorm=31.946, clip=0.000, oom=0.000, loss_scale=16.000, wall=2997, train_wall=2818, accuracy=0.900215\n",
      "| epoch 006:    300 / 305 loss=0.369, nll_loss=0.015, ppl=1.01, wps=488, ups=1, wpb=771.857, bsz=31.990, num_updates=1823, lr=4.12982e-06, gnorm=31.723, clip=0.000, oom=0.000, loss_scale=16.000, wall=3036, train_wall=2857, accuracy=0.900197\n",
      "| epoch 006 | loss 0.369 | nll_loss 0.015 | ppl 1.01 | wps 488 | ups 1 | wpb 770.593 | bsz 31.938 | num_updates 1827 | lr 4.11579e-06 | gnorm 31.748 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 3042 | train_wall 2862 | accuracy 0.900421\n",
      "| epoch 006 | valid on 'valid' subset | loss 1.113 | nll_loss 0.046 | ppl 1.03 | num_updates 1827 | best_accuracy 0.779575 | accuracy 0.765686\n",
      "| epoch 007:     25 / 305 loss=0.294, nll_loss=0.012, ppl=1.01, wps=489, ups=1, wpb=783.577, bsz=32.000, num_updates=1853, lr=4.02456e-06, gnorm=26.401, clip=0.000, oom=0.000, loss_scale=16.000, wall=3101, train_wall=2903, accuracy=0.919471\n",
      "| epoch 007:     50 / 305 loss=0.296, nll_loss=0.012, ppl=1.01, wps=487, ups=1, wpb=776.686, bsz=32.000, num_updates=1878, lr=3.93684e-06, gnorm=27.284, clip=0.000, oom=0.000, loss_scale=16.000, wall=3141, train_wall=2942, accuracy=0.924632\n",
      "| epoch 007:     75 / 305 loss=0.279, nll_loss=0.011, ppl=1.01, wps=486, ups=1, wpb=778.500, bsz=32.000, num_updates=1903, lr=3.84912e-06, gnorm=28.362, clip=0.000, oom=0.000, loss_scale=16.000, wall=3181, train_wall=2982, accuracy=0.929276\n",
      "| epoch 007:    100 / 305 loss=0.289, nll_loss=0.012, ppl=1.01, wps=487, ups=1, wpb=775.436, bsz=32.000, num_updates=1928, lr=3.7614e-06, gnorm=29.335, clip=0.000, oom=0.000, loss_scale=16.000, wall=3220, train_wall=3020, accuracy=0.92729\n",
      "| epoch 007:    125 / 305 loss=0.282, nll_loss=0.012, ppl=1.01, wps=489, ups=1, wpb=774.643, bsz=32.000, num_updates=1953, lr=3.67368e-06, gnorm=29.005, clip=0.000, oom=0.000, loss_scale=16.000, wall=3259, train_wall=3058, accuracy=0.929067\n",
      "| epoch 007:    150 / 305 loss=0.281, nll_loss=0.012, ppl=1.01, wps=490, ups=1, wpb=774.563, bsz=32.000, num_updates=1978, lr=3.58596e-06, gnorm=29.416, clip=0.000, oom=0.000, loss_scale=16.000, wall=3298, train_wall=3097, accuracy=0.929222\n",
      "| epoch 007:    175 / 305 loss=0.281, nll_loss=0.012, ppl=1.01, wps=490, ups=1, wpb=773.756, bsz=32.000, num_updates=2003, lr=3.49825e-06, gnorm=29.437, clip=0.000, oom=0.000, loss_scale=16.000, wall=3337, train_wall=3135, accuracy=0.928622\n",
      "| epoch 007:    200 / 305 loss=0.284, nll_loss=0.012, ppl=1.01, wps=490, ups=1, wpb=772.323, bsz=32.000, num_updates=2028, lr=3.41053e-06, gnorm=29.451, clip=0.000, oom=0.000, loss_scale=16.000, wall=3376, train_wall=3174, accuracy=0.927861\n",
      "| epoch 007:    225 / 305 loss=0.275, nll_loss=0.011, ppl=1.01, wps=491, ups=1, wpb=772.602, bsz=32.000, num_updates=2053, lr=3.32281e-06, gnorm=29.039, clip=0.000, oom=0.000, loss_scale=16.000, wall=3415, train_wall=3212, accuracy=0.929342\n",
      "| epoch 007:    250 / 305 loss=0.283, nll_loss=0.012, ppl=1.01, wps=490, ups=1, wpb=770.880, bsz=31.988, num_updates=2078, lr=3.23509e-06, gnorm=30.076, clip=0.000, oom=0.000, loss_scale=16.000, wall=3454, train_wall=3250, accuracy=0.928136\n",
      "| epoch 007:    275 / 305 loss=0.286, nll_loss=0.012, ppl=1.01, wps=491, ups=1, wpb=771.822, bsz=31.989, num_updates=2103, lr=3.14737e-06, gnorm=29.988, clip=0.000, oom=0.000, loss_scale=16.000, wall=3493, train_wall=3289, accuracy=0.926606\n",
      "| epoch 007:    300 / 305 loss=0.285, nll_loss=0.012, ppl=1.01, wps=491, ups=1, wpb=771.850, bsz=31.990, num_updates=2128, lr=3.05965e-06, gnorm=30.055, clip=0.000, oom=0.000, loss_scale=16.000, wall=3532, train_wall=3327, accuracy=0.926161\n",
      "| epoch 007 | loss 0.285 | nll_loss 0.012 | ppl 1.01 | wps 491 | ups 1 | wpb 770.593 | bsz 31.938 | num_updates 2132 | lr 3.04561e-06 | gnorm 30.177 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 3538 | train_wall 3333 | accuracy 0.926188\n",
      "| epoch 007 | valid on 'valid' subset | loss 1.254 | nll_loss 0.052 | ppl 1.04 | num_updates 2132 | best_accuracy 0.779575 | accuracy 0.761928\n",
      "| epoch 008:     25 / 305 loss=0.235, nll_loss=0.010, ppl=1.01, wps=491, ups=1, wpb=766.962, bsz=32.000, num_updates=2158, lr=2.95439e-06, gnorm=26.465, clip=0.000, oom=0.000, loss_scale=16.000, wall=3596, train_wall=3373, accuracy=0.932692\n",
      "| epoch 008:     50 / 305 loss=0.235, nll_loss=0.010, ppl=1.01, wps=491, ups=1, wpb=768.882, bsz=32.000, num_updates=2183, lr=2.86667e-06, gnorm=28.099, clip=0.000, oom=0.000, loss_scale=16.000, wall=3636, train_wall=3411, accuracy=0.936275\n",
      "| epoch 008:     75 / 305 loss=0.235, nll_loss=0.010, ppl=1.01, wps=491, ups=1, wpb=768.671, bsz=32.000, num_updates=2208, lr=2.77895e-06, gnorm=28.347, clip=0.000, oom=0.000, loss_scale=16.000, wall=3675, train_wall=3450, accuracy=0.938322\n",
      "| epoch 008:    100 / 305 loss=0.239, nll_loss=0.010, ppl=1.01, wps=494, ups=1, wpb=772.158, bsz=32.000, num_updates=2233, lr=2.69123e-06, gnorm=29.692, clip=0.000, oom=0.000, loss_scale=16.000, wall=3713, train_wall=3488, accuracy=0.939047\n",
      "| epoch 008:    125 / 305 loss=0.227, nll_loss=0.009, ppl=1.01, wps=494, ups=1, wpb=770.984, bsz=32.000, num_updates=2258, lr=2.60351e-06, gnorm=28.174, clip=0.000, oom=0.000, loss_scale=16.000, wall=3752, train_wall=3526, accuracy=0.94246\n",
      "| epoch 008:    150 / 305 loss=0.228, nll_loss=0.009, ppl=1.01, wps=494, ups=1, wpb=770.311, bsz=32.000, num_updates=2283, lr=2.51579e-06, gnorm=29.265, clip=0.000, oom=0.000, loss_scale=16.000, wall=3791, train_wall=3565, accuracy=0.941846\n",
      "| epoch 008:    175 / 305 loss=0.221, nll_loss=0.009, ppl=1.01, wps=495, ups=1, wpb=771.795, bsz=32.000, num_updates=2308, lr=2.42807e-06, gnorm=29.245, clip=0.000, oom=0.000, loss_scale=16.000, wall=3830, train_wall=3603, accuracy=0.94407\n",
      "| epoch 008:    200 / 305 loss=0.219, nll_loss=0.009, ppl=1.01, wps=495, ups=1, wpb=771.841, bsz=32.000, num_updates=2333, lr=2.34035e-06, gnorm=29.314, clip=0.000, oom=0.000, loss_scale=16.000, wall=3869, train_wall=3642, accuracy=0.944807\n",
      "| epoch 008:    225 / 305 loss=0.220, nll_loss=0.009, ppl=1.01, wps=494, ups=1, wpb=770.699, bsz=31.987, num_updates=2358, lr=2.25263e-06, gnorm=29.763, clip=0.000, oom=0.000, loss_scale=16.000, wall=3908, train_wall=3680, accuracy=0.944391\n",
      "| epoch 008:    250 / 305 loss=0.222, nll_loss=0.009, ppl=1.01, wps=494, ups=1, wpb=771.486, bsz=31.988, num_updates=2383, lr=2.16491e-06, gnorm=30.217, clip=0.000, oom=0.000, loss_scale=16.000, wall=3948, train_wall=3718, accuracy=0.943829\n",
      "| epoch 008:    275 / 305 loss=0.223, nll_loss=0.009, ppl=1.01, wps=494, ups=1, wpb=771.569, bsz=31.989, num_updates=2408, lr=2.07719e-06, gnorm=30.277, clip=0.000, oom=0.000, loss_scale=16.000, wall=3987, train_wall=3757, accuracy=0.942915\n",
      "| epoch 008:    300 / 305 loss=0.231, nll_loss=0.010, ppl=1.01, wps=495, ups=1, wpb=771.940, bsz=31.990, num_updates=2433, lr=1.98947e-06, gnorm=31.258, clip=0.000, oom=0.000, loss_scale=16.000, wall=4026, train_wall=3795, accuracy=0.940181\n",
      "| epoch 008 | loss 0.231 | nll_loss 0.010 | ppl 1.01 | wps 494 | ups 1 | wpb 770.593 | bsz 31.938 | num_updates 2437 | lr 1.97544e-06 | gnorm 31.235 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 4031 | train_wall 3801 | accuracy 0.940047\n",
      "| epoch 008 | valid on 'valid' subset | loss 1.296 | nll_loss 0.054 | ppl 1.04 | num_updates 2437 | best_accuracy 0.779575 | accuracy 0.769281\n",
      "| epoch 009:     25 / 305 loss=0.180, nll_loss=0.007, ppl=1.01, wps=490, ups=1, wpb=772.154, bsz=32.000, num_updates=2463, lr=1.88421e-06, gnorm=27.306, clip=0.000, oom=0.000, loss_scale=16.000, wall=4090, train_wall=3841, accuracy=0.951923\n",
      "| epoch 009:     50 / 305 loss=0.187, nll_loss=0.008, ppl=1.01, wps=491, ups=1, wpb=768.863, bsz=32.000, num_updates=2488, lr=1.79649e-06, gnorm=27.836, clip=0.000, oom=0.000, loss_scale=16.000, wall=4129, train_wall=3879, accuracy=0.951593\n",
      "| epoch 009:     75 / 305 loss=0.188, nll_loss=0.008, ppl=1.01, wps=492, ups=1, wpb=770.579, bsz=32.000, num_updates=2513, lr=1.70877e-06, gnorm=27.773, clip=0.000, oom=0.000, loss_scale=16.000, wall=4168, train_wall=3918, accuracy=0.950247\n",
      "| epoch 009:    100 / 305 loss=0.185, nll_loss=0.008, ppl=1.01, wps=495, ups=1, wpb=774.307, bsz=32.000, num_updates=2538, lr=1.62105e-06, gnorm=28.507, clip=0.000, oom=0.000, loss_scale=16.000, wall=4207, train_wall=3956, accuracy=0.952661\n",
      "| epoch 009:    125 / 305 loss=0.193, nll_loss=0.008, ppl=1.01, wps=495, ups=1, wpb=772.952, bsz=31.976, num_updates=2563, lr=1.53333e-06, gnorm=29.068, clip=0.000, oom=0.000, loss_scale=16.000, wall=4246, train_wall=3994, accuracy=0.950608\n",
      "| epoch 009:    150 / 305 loss=0.195, nll_loss=0.008, ppl=1.01, wps=495, ups=1, wpb=773.079, bsz=31.980, num_updates=2588, lr=1.44561e-06, gnorm=32.101, clip=0.000, oom=0.000, loss_scale=16.000, wall=4285, train_wall=4033, accuracy=0.948851\n",
      "| epoch 009:    175 / 305 loss=0.196, nll_loss=0.008, ppl=1.01, wps=494, ups=1, wpb=774.261, bsz=31.983, num_updates=2613, lr=1.35789e-06, gnorm=31.512, clip=0.000, oom=0.000, loss_scale=16.000, wall=4324, train_wall=4072, accuracy=0.947415\n",
      "| epoch 009:    200 / 305 loss=0.194, nll_loss=0.008, ppl=1.01, wps=494, ups=1, wpb=773.478, bsz=31.985, num_updates=2638, lr=1.27018e-06, gnorm=30.808, clip=0.000, oom=0.000, loss_scale=16.000, wall=4364, train_wall=4110, accuracy=0.948981\n",
      "| epoch 009:    225 / 305 loss=0.192, nll_loss=0.008, ppl=1.01, wps=494, ups=1, wpb=773.270, bsz=31.987, num_updates=2663, lr=1.18246e-06, gnorm=30.684, clip=0.000, oom=0.000, loss_scale=16.000, wall=4403, train_wall=4148, accuracy=0.948817\n",
      "| epoch 009:    250 / 305 loss=0.193, nll_loss=0.008, ppl=1.01, wps=494, ups=1, wpb=772.853, bsz=31.988, num_updates=2688, lr=1.09474e-06, gnorm=30.240, clip=0.000, oom=0.000, loss_scale=16.000, wall=4442, train_wall=4187, accuracy=0.947939\n",
      "| epoch 009:    275 / 305 loss=0.195, nll_loss=0.008, ppl=1.01, wps=493, ups=1, wpb=771.775, bsz=31.989, num_updates=2713, lr=1.00702e-06, gnorm=30.350, clip=0.000, oom=0.000, loss_scale=16.000, wall=4481, train_wall=4225, accuracy=0.947786\n",
      "| epoch 009:    300 / 305 loss=0.191, nll_loss=0.008, ppl=1.01, wps=493, ups=1, wpb=772.063, bsz=31.990, num_updates=2738, lr=9.19298e-07, gnorm=29.691, clip=0.000, oom=0.000, loss_scale=16.000, wall=4520, train_wall=4264, accuracy=0.949424\n",
      "| epoch 009 | loss 0.191 | nll_loss 0.008 | ppl 1.01 | wps 493 | ups 1 | wpb 770.593 | bsz 31.938 | num_updates 2742 | lr 9.05263e-07 | gnorm 29.797 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 4526 | train_wall 4270 | accuracy 0.949389\n",
      "| epoch 009 | valid on 'valid' subset | loss 1.385 | nll_loss 0.058 | ppl 1.04 | num_updates 2742 | best_accuracy 0.779575 | accuracy 0.770098\n",
      "| epoch 010:     25 / 305 loss=0.155, nll_loss=0.007, ppl=1, wps=494, ups=1, wpb=762.269, bsz=32.000, num_updates=2768, lr=8.14035e-07, gnorm=26.990, clip=0.000, oom=0.000, loss_scale=16.000, wall=4584, train_wall=4309, accuracy=0.959135\n",
      "| epoch 010:     50 / 305 loss=0.154, nll_loss=0.006, ppl=1, wps=494, ups=1, wpb=769.804, bsz=32.000, num_updates=2793, lr=7.26316e-07, gnorm=25.508, clip=0.000, oom=0.000, loss_scale=16.000, wall=4623, train_wall=4348, accuracy=0.959559\n",
      "| epoch 010:     75 / 305 loss=0.153, nll_loss=0.006, ppl=1, wps=495, ups=1, wpb=770.974, bsz=32.000, num_updates=2818, lr=6.38596e-07, gnorm=25.674, clip=0.000, oom=0.000, loss_scale=16.000, wall=4662, train_wall=4386, accuracy=0.958882\n",
      "| epoch 010:    100 / 305 loss=0.154, nll_loss=0.006, ppl=1, wps=494, ups=1, wpb=772.149, bsz=32.000, num_updates=2843, lr=5.50877e-07, gnorm=26.234, clip=0.000, oom=0.000, loss_scale=16.000, wall=4702, train_wall=4425, accuracy=0.957921\n",
      "| epoch 010:    125 / 305 loss=0.156, nll_loss=0.006, ppl=1, wps=494, ups=1, wpb=771.619, bsz=31.976, num_updates=2868, lr=4.63158e-07, gnorm=26.536, clip=0.000, oom=0.000, loss_scale=16.000, wall=4741, train_wall=4463, accuracy=0.95731\n",
      "| epoch 010:    150 / 305 loss=0.156, nll_loss=0.006, ppl=1, wps=493, ups=1, wpb=771.311, bsz=31.980, num_updates=2893, lr=3.75439e-07, gnorm=27.568, clip=0.000, oom=0.000, loss_scale=16.000, wall=4780, train_wall=4502, accuracy=0.957962\n",
      "| epoch 010:    175 / 305 loss=0.156, nll_loss=0.006, ppl=1, wps=493, ups=1, wpb=771.324, bsz=31.983, num_updates=2918, lr=2.87719e-07, gnorm=29.880, clip=0.000, oom=0.000, loss_scale=16.000, wall=4819, train_wall=4540, accuracy=0.957186\n",
      "| epoch 010:    200 / 305 loss=0.158, nll_loss=0.007, ppl=1, wps=493, ups=1, wpb=771.045, bsz=31.985, num_updates=2943, lr=2e-07, gnorm=29.721, clip=0.000, oom=0.000, loss_scale=16.000, wall=4858, train_wall=4579, accuracy=0.957381\n",
      "| epoch 010:    225 / 305 loss=0.159, nll_loss=0.007, ppl=1, wps=492, ups=1, wpb=769.571, bsz=31.987, num_updates=2968, lr=1.12281e-07, gnorm=30.454, clip=0.000, oom=0.000, loss_scale=16.000, wall=4897, train_wall=4617, accuracy=0.957255\n",
      "| epoch 010:    250 / 305 loss=0.160, nll_loss=0.007, ppl=1, wps=492, ups=1, wpb=769.534, bsz=31.988, num_updates=2993, lr=2.45614e-08, gnorm=30.700, clip=0.000, oom=0.000, loss_scale=16.000, wall=4936, train_wall=4656, accuracy=0.957529\n",
      "| epoch 010 | loss 0.159 | nll_loss 0.007 | ppl 1 | wps 493 | ups 1 | wpb 769.748 | bsz 31.988 | num_updates 3000 | lr 0 | gnorm 30.412 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 4947 | train_wall 4666 | accuracy 0.957955\n",
      "| epoch 010 | valid on 'valid' subset | loss 1.446 | nll_loss 0.060 | ppl 1.04 | num_updates 3000 | best_accuracy 0.779575 | accuracy 0.768464\n",
      "| done training in 4944.3 seconds\n"
     ]
    }
   ],
   "source": [
    "# Finetune\n",
    "!bash finetune.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/CommonsenseQA/fairseq\n",
      "/home/jupyter/CommonsenseQA/fairseq\n"
     ]
    }
   ],
   "source": [
    "%cd /home/jupyter/CommonsenseQA/fairseq\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/CommonsenseQA/fairseq/examples/roberta/commonsense_qa\n",
      "['/home/jupyter/CommonsenseQA', '/opt/anaconda3/lib/python37.zip', '/opt/anaconda3/lib/python3.7', '/opt/anaconda3/lib/python3.7/lib-dynload', '', '/opt/anaconda3/lib/python3.7/site-packages', '/opt/anaconda3/lib/python3.7/site-packages/IPython/extensions', '/home/jupyter/.ipython']\n"
     ]
    }
   ],
   "source": [
    "# Try to resolve import path issues\n",
    "\n",
    "%cd /home/jupyter/CommonsenseQA/fairseq/examples/roberta/commonsense_qa\n",
    "import sys\n",
    "# sys.path.insert(0, '/home/jupyter/CommonsenseQA/fairseq')\n",
    "# sys.path.insert(0, '/home/jupyter/CommonsenseQA/fairseq/examples/roberta/commonsense_qa')\n",
    "print(sys.path)\n",
    "# import examples\n",
    "# from examples.roberta import commonsense_qa\n",
    "import commonsense_qa_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/CommonsenseQA/fairseq/examples/roberta/commonsense_qa\n",
      "loading archive file /home/jupyter/CommonsenseQA/fairseq/checkpoints\n",
      "loading archive file /home/jupyter/CommonsenseQA/data/CommonsenseQA\n",
      "| dictionary: 50265 types\n",
      "Accuracy: 0.782964782964783\n"
     ]
    }
   ],
   "source": [
    "%cd /home/jupyter/CommonsenseQA/fairseq/examples/roberta/commonsense_qa\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from fairseq.models.roberta import RobertaModel\n",
    "# from examples.roberta import commonsense_qa  # load the Commonsense QA task\n",
    "import commonsense_qa_task  # load the Commonsense QA task\n",
    "roberta = RobertaModel.from_pretrained('/home/jupyter/CommonsenseQA/fairseq/checkpoints', 'checkpoint_best.pt', '/home/jupyter/CommonsenseQA/data/CommonsenseQA')\n",
    "print(0)\n",
    "roberta.eval()  # disable dropout\n",
    "print(1)\n",
    "roberta.cuda()  # use the GPU (optional)\n",
    "nsamples, ncorrect = 0, 0\n",
    "wrong = []\n",
    "with open('/home/jupyter/CommonsenseQA/data/CommonsenseQA/valid.jsonl') as h:\n",
    "    print(3)\n",
    "    for line in h:\n",
    "        example = json.loads(line)\n",
    "        scores = []\n",
    "        for choice in example['question']['choices']:\n",
    "            input = roberta.encode(\n",
    "                'Q: ' + example['question']['stem'],\n",
    "                'A: ' + choice['text'],\n",
    "                no_separator=True\n",
    "            )\n",
    "            score = roberta.predict('sentence_classification_head', input, return_logits=True)\n",
    "            scores.append(score)\n",
    "#             print(choice['label'], score.data.item())\n",
    "\n",
    "        pred = torch.cat(scores).argmax()\n",
    "#         print('pred: ', chr(ord('A') + pred), 'correct: ', example['answerKey'])\n",
    "        answer = ord(example['answerKey']) - ord('A')\n",
    "        nsamples += 1\n",
    "        if pred == answer:\n",
    "            ncorrect += 1\n",
    "        else:\n",
    "            example['predicted'] = chr(ord('A') + pred)\n",
    "            example['scores'] = {chr(ord('A') + i): s.data.item() for (i, s) in enumerate(scores)}\n",
    "            wrong.append(json.dumps(example))\n",
    "\n",
    "print(4)\n",
    "# Write a file with JSON lines for wrong predictions\n",
    "with open('/home/jupyter/CommonsenseQA/wrong_preds.jsonl', 'w') as f:\n",
    "    f.write('\\n'.join(wrong))\n",
    "\n",
    "print(5)\n",
    "print(f'Accuracy: {ncorrect}/{nsamples} = {ncorrect / float(nsamples)}')\n",
    "# Accuracy: 0.7846027846027847"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy: 0.782964782964783"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrong_preds jsonl2tsv\n",
    "import json\n",
    "\n",
    "choice_chars = ['A', 'B', 'C', 'D', 'E']\n",
    "tsvlines = ['id\\tquestion_concept\\tquestion\\tchoiceA\\tscoreA\\tchoiceB\\tscoreB\\tchoiceC\\tscoreC\\tchoiceD\\tscoreD\\tchoiceE\\tscoreE\\tanswer\\tpredicted']\n",
    "with open('/home/jupyter/CommonsenseQA/wrong_preds.jsonl') as f:\n",
    "    for line in f:\n",
    "        q = json.loads(line)\n",
    "        l = []\n",
    "        l.append(q['id'])\n",
    "        l.append(q['question']['question_concept'])\n",
    "        l.append(q['question']['stem'])\n",
    "\n",
    "        choices = {}\n",
    "        for c in q['question']['choices']:\n",
    "            choices[c['label']] = f\"{c['text']}\\t{round(q['scores'][c['label']], 4)}\"\n",
    "        # To make sure TSV has choices in the order A,B,C,D,E\n",
    "        for c in choice_chars:\n",
    "            l.append(choices[c])\n",
    "\n",
    "        l.append(q['answerKey'])\n",
    "        l.append(q['predicted'])\n",
    "        tsvlines.append('\\t'.join(l))\n",
    "        # print('\\n'.join(tsvlines))\n",
    "        # break\n",
    "\n",
    "with open('/home/jupyter/CommonsenseQA/wrong_preds.tsv', 'w') as f:\n",
    "    f.write('\\n'.join(tsvlines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Requirement already satisfied: spacy in c:\\python37\\lib\\site-packages (2.2.4)\nRequirement already satisfied: thinc==7.4.0 in c:\\python37\\lib\\site-packages (from spacy) (7.4.0)\nRequirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\python37\\lib\\site-packages (from spacy) (1.0.2)\nRequirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\python37\\lib\\site-packages (from spacy) (1.1.3)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\python37\\lib\\site-packages (from spacy) (1.0.2)\nRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\python37\\lib\\site-packages (from spacy) (0.6.0)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\python37\\lib\\site-packages (from spacy) (2.22.0)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\python37\\lib\\site-packages (from spacy) (2.0.3)\nRequirement already satisfied: numpy>=1.15.0 in c:\\python37\\lib\\site-packages (from spacy) (1.17.4)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\python37\\lib\\site-packages (from spacy) (3.0.2)\nRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\python37\\lib\\site-packages (from spacy) (1.0.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\python37\\lib\\site-packages (from spacy) (4.43.0)\nRequirement already satisfied: setuptools in c:\\python37\\lib\\site-packages (from spacy) (41.6.0)\nRequirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\python37\\lib\\site-packages (from spacy) (0.4.1)\nRequirement already satisfied: certifi>=2017.4.17 in c:\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.9.11)\nRequirement already satisfied: idna<2.9,>=2.5 in c:\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.7)\nRequirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\users\\shatru\\appdata\\roaming\\python\\python37\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (0.23)\nRequirement already satisfied: zipp>=0.5 in c:\\users\\shatru\\appdata\\roaming\\python\\python37\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (0.6.0)\nRequirement already satisfied: more-itertools in c:\\users\\shatru\\appdata\\roaming\\python\\python37\\site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (7.2.0)\nYou are using pip version 19.0.3, however version 20.0.2 is available.\nYou should consider upgrading via the 'python -m pip install --upgrade pip' command.\nCollecting en_core_web_lg==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz#egg=en_core_web_lg==2.2.5\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9MB)\nRequirement already satisfied: spacy>=2.2.2 in c:\\python37\\lib\\site-packages (from en_core_web_lg==2.2.5) (2.2.4)\nRequirement already satisfied: numpy>=1.15.0 in c:\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.17.4)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.22.0)\nRequirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\nRequirement already satisfied: thinc==7.4.0 in c:\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.3)\nRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\nRequirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.2)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.43.0)\nRequirement already satisfied: setuptools in c:\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (41.6.0)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\nRequirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\nRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.6.0)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\nRequirement already satisfied: idna<2.9,>=2.5 in c:\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.8)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.25.7)\nRequirement already satisfied: certifi>=2017.4.17 in c:\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2019.9.11)\nRequirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\users\\shatru\\appdata\\roaming\\python\\python37\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (0.23)\nRequirement already satisfied: zipp>=0.5 in c:\\users\\shatru\\appdata\\roaming\\python\\python37\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (0.6.0)\nRequirement already satisfied: more-itertools in c:\\users\\shatru\\appdata\\roaming\\python\\python37\\site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (7.2.0)\nInstalling collected packages: en-core-web-lg\n  Running setup.py install for en-core-web-lg: started\n    Running setup.py install for en-core-web-lg: finished with status 'done'\nSuccessfully installed en-core-web-lg-2.2.5\n✔ Download and installation successful\nYou can now load the model via spacy.load('en_core_web_lg')\nYou are using pip version 19.0.3, however version 20.0.2 is available.\nYou should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
    }
   ],
   "source": [
    "# analysing proper nouns in the validation dataset\n",
    "!pip install spacy[cuda100]\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_lg\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "bank\nlibrary\ndepartment\nstore\nmall\nnew\nyork\n"
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "Object of type Span is not JSON serializable",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-394aaa7dc4c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'proper_nouns'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mnew_valid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/CommonsenseQA/data/CommonsenseQA/valid-propn.jsonl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python37\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mindent\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mseparators\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m         default is None and not sort_keys and not kw):\n\u001b[1;32m--> 231\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python37\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[1;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python37\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[1;32mC:\\Python37\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m--> 179\u001b[1;33m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[0;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type Span is not JSON serializable"
     ]
    }
   ],
   "source": [
    "# analysing proper nouns in the validation dataset\n",
    "import json\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# def on_match(matcher, doc, id, matches):\n",
    "#     print('Matched!', matches)\n",
    "\n",
    "pattern = [{'POS': 'PROPN'}]  # look for proper nouns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# matcher.add(\"PropNounsInCQA\", [pattern], on_match=on_match)  # matcher.add expects a list of list\n",
    "matcher.add(\"PropNounsInCQA\", [pattern])  # matcher.add expects a list of list\n",
    "\n",
    "new_valid = []\n",
    "base_path = \"D:/workspace/ASU/Courses/Spring-2020/CSE-576-Topics-in-Natural-Language-Processing/Project-COMMONSENSEQA\"\n",
    "with open(base_path + '/CommonsenseQA/data/CommonsenseQA/valid.jsonl') as f:\n",
    "    for line in f:\n",
    "        q = json.loads(line)\n",
    "        l = []\n",
    "        l.append(q['question']['question_concept'])\n",
    "        l.append(q['question']['stem'])\n",
    "        l += [c['text'] for c in q['question']['choices']]\n",
    "        doc = nlp(' '.join(l))  # get POS tags for concept + question + choices\n",
    "        matches = matcher(doc)\n",
    "        if matches:\n",
    "            # q['has_propn'] = True\n",
    "            q['proper_nouns'] = []\n",
    "        print(doc[:])\n",
    "        for m in matches:\n",
    "            print(doc[m[1]:m[2]])\n",
    "            q['proper_nouns'].append(doc[m[1]:m[2]])\n",
    "\n",
    "        new_valid.append(json.dumps(q))\n",
    "\n",
    "with open(base_path + '/CommonsenseQA/data/CommonsenseQA/valid-propn.jsonl', 'w') as f:\n",
    "    f.write('\\n'.join(new_valid))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}