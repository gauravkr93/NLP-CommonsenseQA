{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before running this notebook, make sure to upload the `CommonsenseQA` folder as a zipped file to the working directory\n",
    "- `/home/jupyter/` in case of Google AI platform notebooks.\n",
    "- `/content/` in case of Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May  3 05:22:06 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.87.01    Driver Version: 418.87.01    CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   48C    P0    42W /  70W |     10MiB / 15079MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Confirm GPU\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  CommonsenseQA.zip\n",
      "   creating: CommonsenseQA/\n",
      "  inflating: CommonsenseQA/convert_jsonl2tsv.py  \n",
      "   creating: CommonsenseQA/data/\n",
      "   creating: CommonsenseQA/data/CommonsenseQA/\n",
      "  inflating: CommonsenseQA/data/CommonsenseQA/dict.txt  \n",
      "  inflating: CommonsenseQA/data/CommonsenseQA/test.jsonl  \n",
      "  inflating: CommonsenseQA/data/CommonsenseQA/train.jsonl  \n",
      "  inflating: CommonsenseQA/data/CommonsenseQA/valid-propn.jsonl  \n",
      "  inflating: CommonsenseQA/data/CommonsenseQA/valid.jsonl  \n",
      "   creating: CommonsenseQA/fairseq/\n",
      "   creating: CommonsenseQA/fairseq/checkpoints/\n",
      " extracting: CommonsenseQA/fairseq/checkpoints/.gitkeep  \n",
      "   creating: CommonsenseQA/fairseq/examples/\n",
      "   creating: CommonsenseQA/fairseq/examples/roberta/\n",
      "   creating: CommonsenseQA/fairseq/examples/roberta/commonsense_qa/\n",
      "  inflating: CommonsenseQA/fairseq/examples/roberta/commonsense_qa/commonsense_qa_task.py  \n",
      "  inflating: CommonsenseQA/fairseq/examples/roberta/commonsense_qa/download_cqa_data.sh  \n",
      "  inflating: CommonsenseQA/fairseq/examples/roberta/commonsense_qa/README.md  \n",
      "  inflating: CommonsenseQA/fairseq/examples/roberta/commonsense_qa/__init__.py  \n",
      "  inflating: CommonsenseQA/finetune.sh  \n",
      "  inflating: CommonsenseQA/wrong_preds.jsonl  \n",
      "  inflating: CommonsenseQA/wrong_preds.tsv  \n"
     ]
    }
   ],
   "source": [
    "# Unzip code\n",
    "!unzip CommonsenseQA.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fairseq in /opt/anaconda3/lib/python3.7/site-packages (0.9.0)\n",
      "Requirement already satisfied: regex in /opt/anaconda3/lib/python3.7/site-packages (from fairseq) (2019.8.19)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.7/site-packages (from fairseq) (4.42.0)\n",
      "Requirement already satisfied: cffi in /opt/anaconda3/lib/python3.7/site-packages (from fairseq) (1.13.0)\n",
      "Requirement already satisfied: sacrebleu in /opt/anaconda3/lib/python3.7/site-packages (from fairseq) (1.4.4)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.7/site-packages (from fairseq) (1.18.1)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.7/site-packages (from fairseq) (1.4.0)\n",
      "Requirement already satisfied: cython in /opt/anaconda3/lib/python3.7/site-packages (from fairseq) (0.29.14)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.7/site-packages (from cffi->fairseq) (2.19)\n",
      "Requirement already satisfied: typing in /opt/anaconda3/lib/python3.7/site-packages (from sacrebleu->fairseq) (3.7.4.1)\n",
      "Requirement already satisfied: portalocker in /opt/anaconda3/lib/python3.7/site-packages (from sacrebleu->fairseq) (1.5.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-03-13 06:14:29--  https://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.20.22.166, 104.20.6.166, 2606:4700:10::6814:6a6, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.20.22.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 655283069 (625M) [application/gzip]\n",
      "Saving to: ‘roberta.large.tar.gz’\n",
      "\n",
      "roberta.large.tar.g 100%[===================>] 624.93M  31.1MB/s    in 22s     \n",
      "\n",
      "2020-03-13 06:14:51 (29.0 MB/s) - ‘roberta.large.tar.gz’ saved [655283069/655283069]\n",
      "\n",
      "roberta.large/\n",
      "roberta.large/dict.txt\n",
      "roberta.large/model.pt\n",
      "roberta.large/NOTE\n"
     ]
    }
   ],
   "source": [
    "# Download roberta model\n",
    "!wget -O roberta.large.tar.gz https://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz\n",
    "!tar -xvzf roberta.large.tar.gz\n",
    "\n",
    "# !wget -O /content/CommonsenseQA/roberta.base.tar.gz https://dl.fbaipublicfiles.com/fairseq/models/roberta.base.tar.gz\n",
    "# !tar -xvzf /content/CommonsenseQA/roberta.base.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/CommonsenseQA\n",
      "/home/jupyter/CommonsenseQA\n"
     ]
    }
   ],
   "source": [
    "%cd /home/jupyter/CommonsenseQA\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing finetune.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile finetune.sh\n",
    "#!/bin/bash\n",
    "\n",
    "## Write the finetuning part to a bash script file\n",
    "# Modified following from the original script to get it to run on Google AI platform and Colab\n",
    "# - Set MAX_SENTENCES=8\n",
    "# - Added --update-freq 4\n",
    "\n",
    "MAX_UPDATES=3000      # Number of training steps.\n",
    "WARMUP_UPDATES=150    # Linearly increase LR over this many steps.\n",
    "LR=1e-05              # Peak LR for polynomial LR scheduler.\n",
    "MAX_SENTENCES=8      # Batch size.\n",
    "SEED=23                # Random seed.\n",
    "\n",
    "BASEDIR=/home/jupyter\n",
    "# CQA_PATH=/content/CommonsenseQA # For Google Colab\n",
    "CQA_PATH=$BASEDIR/CommonsenseQA # For Kaggle\n",
    "ROBERTA_PATH=${BASEDIR}/roberta.large/model.pt\n",
    "DATA_DIR=${CQA_PATH}/data/CommonsenseQA\n",
    "\n",
    "# we use the --user-dir option to load the task from\n",
    "# the examples/roberta/commonsense_qa directory:\n",
    "FAIRSEQ_PATH=${CQA_PATH}/fairseq\n",
    "FAIRSEQ_USER_DIR=${FAIRSEQ_PATH}/examples/roberta/commonsense_qa\n",
    "\n",
    "cd $FAIRSEQ_PATH\n",
    "CUDA_VISIBLE_DEVICES=0 fairseq-train --fp16 --ddp-backend=no_c10d \\\n",
    "    $DATA_DIR \\\n",
    "    --update-freq 4 \\\n",
    "    --save-dir ./checkpoints \\\n",
    "    --user-dir $FAIRSEQ_USER_DIR \\\n",
    "    --restore-file $ROBERTA_PATH \\\n",
    "    --reset-optimizer --reset-dataloader --reset-meters \\\n",
    "    --no-epoch-checkpoints --no-last-checkpoints --no-save-optimizer-state \\\n",
    "    --best-checkpoint-metric accuracy --maximize-best-checkpoint-metric \\\n",
    "    --task commonsense_qa --init-token 0 --bpe gpt2 \\\n",
    "    --arch roberta_large --max-positions 512 \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --criterion sentence_ranking --num-classes 5 \\\n",
    "    --optimizer adam --adam-betas '(0.9, 0.98)' --adam-eps 1e-06 --clip-norm 0.0 \\\n",
    "    --lr-scheduler polynomial_decay --lr $LR \\\n",
    "    --warmup-updates $WARMUP_UPDATES --total-num-update $MAX_UPDATES \\\n",
    "    --max-sentences $MAX_SENTENCES \\\n",
    "    --max-update $MAX_UPDATES \\\n",
    "    --log-format simple --log-interval 25 \\\n",
    "    --seed $SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, arch='roberta_large', attention_dropout=0.1, best_checkpoint_metric='accuracy', bpe='gpt2', bucket_cap_mb=25, clip_norm=0.0, cpu=False, criterion='sentence_ranking', curriculum=0, data='/home/jupyter/CommonsenseQA/data/CommonsenseQA', dataset_impl=None, ddp_backend='no_c10d', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=24, encoder_layers_to_keep=None, end_learning_rate=0.0, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gpt2_encoder_json='https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', gpt2_vocab_bpe='https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe', init_token=0, keep_interval_updates=-1, keep_last_epochs=-1, log_format='simple', log_interval=25, lr=[1e-05], lr_scheduler='polynomial_decay', max_epoch=0, max_positions=512, max_sentences=8, max_sentences_valid=8, max_tokens=None, max_tokens_valid=None, max_update=3000, maximize_best_checkpoint_metric=True, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, no_epoch_checkpoints=True, no_last_checkpoints=True, no_progress_bar=False, no_save=False, no_save_optimizer_state=True, num_classes=5, num_workers=1, optimizer='adam', optimizer_overrides='{}', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, required_batch_size_multiple=8, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/jupyter/roberta.large/model.pt', save_dir='./checkpoints', save_interval=1, save_interval_updates=0, save_predictions=None, seed=23, sentence_avg=False, skip_invalid_size_inputs_valid_test=False, task='commonsense_qa', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, total_num_update=3000, train_subset='train', update_freq=[4], use_bmuf=False, user_dir='/home/jupyter/CommonsenseQA/fairseq/examples/roberta/commonsense_qa', valid_subset='valid', validate_interval=1, warmup_updates=150, weight_decay=0.01)\n",
      "| dictionary: 50265 types\n",
      "| Loaded valid with 1221 samples\n",
      "RobertaModel(\n",
      "  (decoder): RobertaEncoder(\n",
      "    (sentence_encoder): TransformerSentenceEncoder(\n",
      "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
      "      (embed_positions): LearnedPositionalEmbedding(514, 1024, padding_idx=1)\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (6): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (8): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (9): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (10): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (11): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (12): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (13): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (14): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (15): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (16): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (17): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (18): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (19): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (20): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (21): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (22): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (23): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (emb_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (lm_head): RobertaLMHead(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (classification_heads): ModuleDict(\n",
      "    (sentence_classification_head): RobertaClassificationHead(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (out_proj): Linear(in_features=1024, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "| model roberta_large, criterion SentenceRankingCriterion\n",
      "| num. model params: 356461658 (num. trained: 356461658)\n",
      "| training on 1 GPUs\n",
      "| max tokens per GPU = None and max sentences per GPU = 8\n",
      "Overwriting classification_heads.sentence_classification_head.dense.weight\n",
      "Overwriting classification_heads.sentence_classification_head.dense.bias\n",
      "Overwriting classification_heads.sentence_classification_head.out_proj.weight\n",
      "Overwriting classification_heads.sentence_classification_head.out_proj.bias\n",
      "| loaded checkpoint /home/jupyter/roberta.large/model.pt (epoch 0 @ 0 updates)\n",
      "| loading train data for epoch 0\n",
      "| Loaded train with 9741 samples\n",
      "| epoch 001:     25 / 305 loss=2.322, nll_loss=0.096, ppl=1.07, wps=521, ups=1, wpb=775.115, bsz=32.000, num_updates=26, lr=1.73333e-06, gnorm=3.140, clip=0.000, oom=0.000, loss_scale=128.000, wall=48, train_wall=39, accuracy=0.217548\n",
      "| epoch 001:     50 / 305 loss=2.322, nll_loss=0.097, ppl=1.07, wps=515, ups=1, wpb=768.333, bsz=32.000, num_updates=51, lr=3.4e-06, gnorm=4.596, clip=0.000, oom=0.000, loss_scale=128.000, wall=85, train_wall=75, accuracy=0.235294\n",
      "| epoch 001:     75 / 305 loss=2.319, nll_loss=0.096, ppl=1.07, wps=515, ups=1, wpb=770.066, bsz=32.000, num_updates=76, lr=5.06667e-06, gnorm=6.079, clip=0.000, oom=0.000, loss_scale=128.000, wall=123, train_wall=112, accuracy=0.244655\n",
      "| epoch 001:    100 / 305 loss=2.315, nll_loss=0.096, ppl=1.07, wps=514, ups=1, wpb=769.713, bsz=32.000, num_updates=101, lr=6.73333e-06, gnorm=8.267, clip=0.000, oom=0.000, loss_scale=128.000, wall=160, train_wall=149, accuracy=0.252475\n",
      "| epoch 001:    125 / 305 loss=2.297, nll_loss=0.095, ppl=1.07, wps=514, ups=1, wpb=770.413, bsz=32.000, num_updates=126, lr=8.4e-06, gnorm=14.985, clip=0.000, oom=0.000, loss_scale=128.000, wall=198, train_wall=186, accuracy=0.263641\n",
      "| WARNING: overflow detected, setting loss scale to: 64.0\n",
      "| epoch 001:    150 / 305 loss=2.264, nll_loss=0.094, ppl=1.07, wps=511, ups=1, wpb=771.707, bsz=32.000, num_updates=150, lr=1e-05, gnorm=21.471, clip=0.000, oom=0.000, loss_scale=64.000, wall=235, train_wall=223, accuracy=0.280208\n",
      "| WARNING: overflow detected, setting loss scale to: 32.0\n",
      "| epoch 001:    175 / 305 loss=2.212, nll_loss=0.092, ppl=1.07, wps=509, ups=1, wpb=771.155, bsz=31.983, num_updates=174, lr=9.91579e-06, gnorm=28.260, clip=0.000, oom=0.000, loss_scale=32.000, wall=273, train_wall=260, accuracy=0.309434\n",
      "| epoch 001:    200 / 305 loss=2.157, nll_loss=0.089, ppl=1.06, wps=510, ups=1, wpb=771.347, bsz=31.985, num_updates=199, lr=9.82807e-06, gnorm=32.936, clip=0.000, oom=0.000, loss_scale=32.000, wall=310, train_wall=297, accuracy=0.3315\n",
      "| epoch 001:    225 / 305 loss=2.108, nll_loss=0.087, ppl=1.06, wps=511, ups=1, wpb=771.839, bsz=31.987, num_updates=224, lr=9.74035e-06, gnorm=34.166, clip=0.000, oom=0.000, loss_scale=32.000, wall=348, train_wall=334, accuracy=0.352966\n",
      "| epoch 001:    250 / 305 loss=2.051, nll_loss=0.085, ppl=1.06, wps=511, ups=1, wpb=772.221, bsz=31.988, num_updates=249, lr=9.65263e-06, gnorm=34.524, clip=0.000, oom=0.000, loss_scale=32.000, wall=385, train_wall=371, accuracy=0.375267\n",
      "| epoch 001:    275 / 305 loss=2.001, nll_loss=0.083, ppl=1.06, wps=512, ups=1, wpb=771.956, bsz=31.989, num_updates=274, lr=9.56491e-06, gnorm=34.217, clip=0.000, oom=0.000, loss_scale=32.000, wall=422, train_wall=407, accuracy=0.395322\n",
      "| epoch 001:    300 / 305 loss=1.955, nll_loss=0.081, ppl=1.06, wps=512, ups=1, wpb=772.164, bsz=31.990, num_updates=299, lr=9.47719e-06, gnorm=34.207, clip=0.000, oom=0.000, loss_scale=32.000, wall=460, train_wall=444, accuracy=0.412128\n",
      "| epoch 001 | loss 1.947 | nll_loss 0.081 | ppl 1.06 | wps 512 | ups 1 | wpb 770.574 | bsz 31.937 | num_updates 303 | lr 9.46316e-06 | gnorm 34.197 | clip 0.000 | oom 0.000 | loss_scale 32.000 | wall 465 | train_wall 449 | accuracy 0.414695\n",
      "| epoch 001 | valid on 'valid' subset | loss 1.094 | nll_loss 0.046 | ppl 1.03 | num_updates 303 | accuracy 0.733007\n",
      "| saved checkpoint ./checkpoints/checkpoint_best.pt (epoch 1 @ 303 updates) (writing took 3.848008632659912 seconds)\n",
      "| epoch 002:     25 / 305 loss=1.292, nll_loss=0.054, ppl=1.04, wps=511, ups=1, wpb=767.038, bsz=32.000, num_updates=329, lr=9.37193e-06, gnorm=42.538, clip=0.000, oom=0.000, loss_scale=32.000, wall=525, train_wall=488, accuracy=0.628606\n",
      "| epoch 002:     50 / 305 loss=1.325, nll_loss=0.055, ppl=1.04, wps=515, ups=1, wpb=769.471, bsz=32.000, num_updates=354, lr=9.28421e-06, gnorm=41.430, clip=0.000, oom=0.000, loss_scale=32.000, wall=562, train_wall=524, accuracy=0.620711\n",
      "| epoch 002:     75 / 305 loss=1.308, nll_loss=0.055, ppl=1.04, wps=515, ups=1, wpb=766.053, bsz=32.000, num_updates=379, lr=9.19649e-06, gnorm=39.183, clip=0.000, oom=0.000, loss_scale=32.000, wall=599, train_wall=560, accuracy=0.622944\n",
      "| epoch 002:    100 / 305 loss=1.293, nll_loss=0.054, ppl=1.04, wps=516, ups=1, wpb=770.713, bsz=32.000, num_updates=404, lr=9.10877e-06, gnorm=36.853, clip=0.000, oom=0.000, loss_scale=32.000, wall=637, train_wall=598, accuracy=0.634592\n",
      "| epoch 002:    125 / 305 loss=1.294, nll_loss=0.054, ppl=1.04, wps=516, ups=1, wpb=769.238, bsz=32.000, num_updates=429, lr=9.02105e-06, gnorm=36.025, clip=0.000, oom=0.000, loss_scale=32.000, wall=674, train_wall=634, accuracy=0.636161\n",
      "| epoch 002:    150 / 305 loss=1.281, nll_loss=0.053, ppl=1.04, wps=516, ups=1, wpb=769.152, bsz=32.000, num_updates=454, lr=8.93333e-06, gnorm=35.252, clip=0.000, oom=0.000, loss_scale=32.000, wall=711, train_wall=671, accuracy=0.64197\n",
      "| epoch 002:    175 / 305 loss=1.272, nll_loss=0.053, ppl=1.04, wps=517, ups=1, wpb=770.636, bsz=32.000, num_updates=479, lr=8.84561e-06, gnorm=34.465, clip=0.000, oom=0.000, loss_scale=32.000, wall=749, train_wall=707, accuracy=0.645241\n",
      "| epoch 002:    200 / 305 loss=1.269, nll_loss=0.053, ppl=1.04, wps=518, ups=1, wpb=772.234, bsz=32.000, num_updates=504, lr=8.75789e-06, gnorm=34.070, clip=0.000, oom=0.000, loss_scale=32.000, wall=786, train_wall=744, accuracy=0.645211\n",
      "| epoch 002:    225 / 305 loss=1.259, nll_loss=0.052, ppl=1.04, wps=518, ups=1, wpb=771.155, bsz=31.987, num_updates=529, lr=8.67018e-06, gnorm=34.250, clip=0.000, oom=0.000, loss_scale=32.000, wall=823, train_wall=780, accuracy=0.650436\n",
      "| epoch 002:    250 / 305 loss=1.259, nll_loss=0.052, ppl=1.04, wps=519, ups=1, wpb=772.351, bsz=31.988, num_updates=554, lr=8.58246e-06, gnorm=34.488, clip=0.000, oom=0.000, loss_scale=32.000, wall=860, train_wall=817, accuracy=0.652759\n",
      "| epoch 002:    275 / 305 loss=1.254, nll_loss=0.052, ppl=1.04, wps=519, ups=1, wpb=771.808, bsz=31.989, num_updates=579, lr=8.49474e-06, gnorm=35.041, clip=0.000, oom=0.000, loss_scale=32.000, wall=897, train_wall=853, accuracy=0.65568\n",
      "| epoch 002:    300 / 305 loss=1.245, nll_loss=0.052, ppl=1.04, wps=519, ups=1, wpb=771.668, bsz=31.990, num_updates=604, lr=8.40702e-06, gnorm=37.748, clip=0.000, oom=0.000, loss_scale=32.000, wall=934, train_wall=890, accuracy=0.660297\n",
      "| epoch 002 | loss 1.244 | nll_loss 0.052 | ppl 1.04 | wps 519 | ups 1 | wpb 770.593 | bsz 31.938 | num_updates 608 | lr 8.39298e-06 | gnorm 37.876 | clip 0.000 | oom 0.000 | loss_scale 32.000 | wall 939 | train_wall 895 | accuracy 0.660507\n",
      "| epoch 002 | valid on 'valid' subset | loss 0.935 | nll_loss 0.039 | ppl 1.03 | num_updates 608 | best_accuracy 0.746895 | accuracy 0.746895\n",
      "| saved checkpoint ./checkpoints/checkpoint_best.pt (epoch 2 @ 608 updates) (writing took 3.8252317905426025 seconds)\n",
      "| epoch 003:     25 / 305 loss=0.920, nll_loss=0.038, ppl=1.03, wps=526, ups=1, wpb=776.192, bsz=32.000, num_updates=634, lr=8.30175e-06, gnorm=54.722, clip=0.000, oom=0.000, loss_scale=32.000, wall=1000, train_wall=933, accuracy=0.772837\n",
      "| epoch 003:     50 / 305 loss=0.949, nll_loss=0.039, ppl=1.03, wps=524, ups=1, wpb=773.294, bsz=31.941, num_updates=659, lr=8.21404e-06, gnorm=49.232, clip=0.000, oom=0.000, loss_scale=32.000, wall=1036, train_wall=969, accuracy=0.750153\n",
      "| epoch 003:     75 / 305 loss=0.922, nll_loss=0.038, ppl=1.03, wps=524, ups=1, wpb=772.237, bsz=31.961, num_updates=684, lr=8.12632e-06, gnorm=44.944, clip=0.000, oom=0.000, loss_scale=32.000, wall=1073, train_wall=1005, accuracy=0.752573\n",
      "| epoch 003:    100 / 305 loss=0.920, nll_loss=0.038, ppl=1.03, wps=524, ups=1, wpb=774.446, bsz=31.970, num_updates=709, lr=8.0386e-06, gnorm=43.775, clip=0.000, oom=0.000, loss_scale=32.000, wall=1110, train_wall=1042, accuracy=0.755962\n",
      "| epoch 003:    125 / 305 loss=0.914, nll_loss=0.038, ppl=1.03, wps=525, ups=1, wpb=775.698, bsz=31.976, num_updates=734, lr=7.95088e-06, gnorm=42.122, clip=0.000, oom=0.000, loss_scale=32.000, wall=1147, train_wall=1078, accuracy=0.756019\n",
      "| epoch 003:    150 / 305 loss=0.905, nll_loss=0.037, ppl=1.03, wps=525, ups=1, wpb=773.987, bsz=31.980, num_updates=759, lr=7.86316e-06, gnorm=41.951, clip=0.000, oom=0.000, loss_scale=32.000, wall=1184, train_wall=1114, accuracy=0.756471\n",
      "| epoch 003:    175 / 305 loss=0.908, nll_loss=0.038, ppl=1.03, wps=524, ups=1, wpb=773.159, bsz=31.983, num_updates=784, lr=7.77544e-06, gnorm=41.078, clip=0.000, oom=0.000, loss_scale=32.000, wall=1221, train_wall=1150, accuracy=0.757506\n",
      "| epoch 003:    200 / 305 loss=0.907, nll_loss=0.038, ppl=1.03, wps=524, ups=1, wpb=772.667, bsz=31.985, num_updates=809, lr=7.68772e-06, gnorm=40.585, clip=0.000, oom=0.000, loss_scale=32.000, wall=1257, train_wall=1186, accuracy=0.75595\n",
      "| epoch 003:    225 / 305 loss=0.916, nll_loss=0.038, ppl=1.03, wps=525, ups=1, wpb=772.854, bsz=31.987, num_updates=834, lr=7.6e-06, gnorm=40.291, clip=0.000, oom=0.000, loss_scale=32.000, wall=1294, train_wall=1222, accuracy=0.7546\n",
      "| epoch 003:    250 / 305 loss=0.908, nll_loss=0.038, ppl=1.03, wps=525, ups=1, wpb=773.773, bsz=31.988, num_updates=859, lr=7.51228e-06, gnorm=40.258, clip=0.000, oom=0.000, loss_scale=32.000, wall=1331, train_wall=1259, accuracy=0.756383\n",
      "| epoch 003:    275 / 305 loss=0.911, nll_loss=0.038, ppl=1.03, wps=524, ups=1, wpb=772.895, bsz=31.989, num_updates=884, lr=7.42456e-06, gnorm=39.608, clip=0.000, oom=0.000, loss_scale=32.000, wall=1368, train_wall=1295, accuracy=0.755012\n",
      "| epoch 003:    300 / 305 loss=0.911, nll_loss=0.038, ppl=1.03, wps=524, ups=1, wpb=772.173, bsz=31.990, num_updates=909, lr=7.33684e-06, gnorm=41.294, clip=0.000, oom=0.000, loss_scale=32.000, wall=1405, train_wall=1331, accuracy=0.753765\n",
      "| epoch 003 | loss 0.912 | nll_loss 0.038 | ppl 1.03 | wps 524 | ups 1 | wpb 770.593 | bsz 31.938 | num_updates 913 | lr 7.32281e-06 | gnorm 41.252 | clip 0.000 | oom 0.000 | loss_scale 32.000 | wall 1410 | train_wall 1336 | accuracy 0.753619\n",
      "| epoch 003 | valid on 'valid' subset | loss 0.899 | nll_loss 0.038 | ppl 1.03 | num_updates 913 | best_accuracy 0.768137 | accuracy 0.768137\n",
      "| saved checkpoint ./checkpoints/checkpoint_best.pt (epoch 3 @ 913 updates) (writing took 3.829596996307373 seconds)\n",
      "| WARNING: overflow detected, setting loss scale to: 16.0\n",
      "| epoch 004:     25 / 305 loss=0.656, nll_loss=0.028, ppl=1.02, wps=500, ups=1, wpb=762.600, bsz=32.000, num_updates=938, lr=7.23509e-06, gnorm=44.815, clip=0.000, oom=0.000, loss_scale=16.000, wall=1469, train_wall=1374, accuracy=0.8275\n",
      "| epoch 004:     50 / 305 loss=0.671, nll_loss=0.028, ppl=1.02, wps=510, ups=1, wpb=760.840, bsz=32.000, num_updates=963, lr=7.14737e-06, gnorm=50.075, clip=0.000, oom=0.000, loss_scale=16.000, wall=1505, train_wall=1410, accuracy=0.823125\n",
      "| epoch 004:     75 / 305 loss=0.670, nll_loss=0.028, ppl=1.02, wps=517, ups=1, wpb=768.333, bsz=32.000, num_updates=988, lr=7.05965e-06, gnorm=48.850, clip=0.000, oom=0.000, loss_scale=16.000, wall=1542, train_wall=1446, accuracy=0.821667\n",
      "| epoch 004:    100 / 305 loss=0.670, nll_loss=0.028, ppl=1.02, wps=519, ups=1, wpb=771.120, bsz=32.000, num_updates=1013, lr=6.97193e-06, gnorm=45.660, clip=0.000, oom=0.000, loss_scale=16.000, wall=1579, train_wall=1483, accuracy=0.822812\n",
      "| epoch 004:    125 / 305 loss=0.658, nll_loss=0.027, ppl=1.02, wps=519, ups=1, wpb=769.960, bsz=32.000, num_updates=1038, lr=6.88421e-06, gnorm=44.363, clip=0.000, oom=0.000, loss_scale=16.000, wall=1616, train_wall=1519, accuracy=0.82675\n",
      "| epoch 004:    150 / 305 loss=0.648, nll_loss=0.027, ppl=1.02, wps=520, ups=1, wpb=769.387, bsz=32.000, num_updates=1063, lr=6.79649e-06, gnorm=44.678, clip=0.000, oom=0.000, loss_scale=16.000, wall=1653, train_wall=1555, accuracy=0.829375\n",
      "| epoch 004:    175 / 305 loss=0.657, nll_loss=0.027, ppl=1.02, wps=521, ups=1, wpb=769.863, bsz=32.000, num_updates=1088, lr=6.70877e-06, gnorm=47.005, clip=0.000, oom=0.000, loss_scale=16.000, wall=1689, train_wall=1591, accuracy=0.826964\n",
      "| epoch 004:    200 / 305 loss=0.654, nll_loss=0.027, ppl=1.02, wps=521, ups=1, wpb=769.405, bsz=32.000, num_updates=1113, lr=6.62105e-06, gnorm=45.882, clip=0.000, oom=0.000, loss_scale=16.000, wall=1726, train_wall=1627, accuracy=0.828281\n",
      "| epoch 004:    225 / 305 loss=0.661, nll_loss=0.027, ppl=1.02, wps=522, ups=1, wpb=770.004, bsz=32.000, num_updates=1138, lr=6.53333e-06, gnorm=44.409, clip=0.000, oom=0.000, loss_scale=16.000, wall=1762, train_wall=1663, accuracy=0.825972\n",
      "| epoch 004:    250 / 305 loss=0.669, nll_loss=0.028, ppl=1.02, wps=522, ups=1, wpb=770.604, bsz=32.000, num_updates=1163, lr=6.44561e-06, gnorm=43.761, clip=0.000, oom=0.000, loss_scale=16.000, wall=1799, train_wall=1700, accuracy=0.823125\n",
      "| epoch 004:    275 / 305 loss=0.675, nll_loss=0.028, ppl=1.02, wps=523, ups=1, wpb=771.873, bsz=32.000, num_updates=1188, lr=6.35789e-06, gnorm=42.741, clip=0.000, oom=0.000, loss_scale=16.000, wall=1836, train_wall=1736, accuracy=0.821477\n",
      "| epoch 004:    300 / 305 loss=0.673, nll_loss=0.028, ppl=1.02, wps=523, ups=1, wpb=771.887, bsz=31.990, num_updates=1213, lr=6.27018e-06, gnorm=42.051, clip=0.000, oom=0.000, loss_scale=16.000, wall=1873, train_wall=1772, accuracy=0.821194\n",
      "| epoch 004 | loss 0.672 | nll_loss 0.028 | ppl 1.02 | wps 523 | ups 1 | wpb 770.592 | bsz 31.938 | num_updates 1217 | lr 6.25614e-06 | gnorm 42.047 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 1878 | train_wall 1777 | accuracy 0.8213\n",
      "| epoch 004 | valid on 'valid' subset | loss 0.962 | nll_loss 0.040 | ppl 1.03 | num_updates 1217 | best_accuracy 0.779575 | accuracy 0.779575\n",
      "| saved checkpoint ./checkpoints/checkpoint_best.pt (epoch 4 @ 1217 updates) (writing took 3.816718339920044 seconds)\n",
      "| epoch 005:     25 / 305 loss=0.484, nll_loss=0.020, ppl=1.01, wps=524, ups=1, wpb=764.154, bsz=32.000, num_updates=1243, lr=6.16491e-06, gnorm=31.952, clip=0.000, oom=0.000, loss_scale=16.000, wall=1937, train_wall=1815, accuracy=0.878606\n",
      "| epoch 005:     50 / 305 loss=0.468, nll_loss=0.019, ppl=1.01, wps=525, ups=1, wpb=774.098, bsz=32.000, num_updates=1268, lr=6.07719e-06, gnorm=33.486, clip=0.000, oom=0.000, loss_scale=16.000, wall=1974, train_wall=1851, accuracy=0.877451\n",
      "| epoch 005:     75 / 305 loss=0.472, nll_loss=0.019, ppl=1.01, wps=526, ups=1, wpb=776.197, bsz=32.000, num_updates=1293, lr=5.98947e-06, gnorm=34.750, clip=0.000, oom=0.000, loss_scale=16.000, wall=2011, train_wall=1888, accuracy=0.877878\n",
      "| epoch 005:    100 / 305 loss=0.500, nll_loss=0.021, ppl=1.01, wps=527, ups=1, wpb=777.248, bsz=32.000, num_updates=1318, lr=5.90175e-06, gnorm=35.419, clip=0.000, oom=0.000, loss_scale=16.000, wall=2048, train_wall=1924, accuracy=0.870359\n",
      "| epoch 005:    125 / 305 loss=0.500, nll_loss=0.021, ppl=1.01, wps=526, ups=1, wpb=776.937, bsz=32.000, num_updates=1343, lr=5.81404e-06, gnorm=37.418, clip=0.000, oom=0.000, loss_scale=16.000, wall=2085, train_wall=1960, accuracy=0.871776\n",
      "| epoch 005:    150 / 305 loss=0.505, nll_loss=0.021, ppl=1.01, wps=526, ups=1, wpb=775.503, bsz=32.000, num_updates=1368, lr=5.72632e-06, gnorm=37.524, clip=0.000, oom=0.000, loss_scale=16.000, wall=2122, train_wall=1997, accuracy=0.87024\n",
      "| epoch 005:    175 / 305 loss=0.495, nll_loss=0.020, ppl=1.01, wps=526, ups=1, wpb=775.455, bsz=32.000, num_updates=1393, lr=5.6386e-06, gnorm=36.329, clip=0.000, oom=0.000, loss_scale=16.000, wall=2159, train_wall=2033, accuracy=0.872337\n",
      "| epoch 005:    200 / 305 loss=0.491, nll_loss=0.020, ppl=1.01, wps=525, ups=1, wpb=774.244, bsz=31.985, num_updates=1418, lr=5.55088e-06, gnorm=36.360, clip=0.000, oom=0.000, loss_scale=16.000, wall=2196, train_wall=2069, accuracy=0.872297\n",
      "| epoch 005:    225 / 305 loss=0.496, nll_loss=0.021, ppl=1.01, wps=525, ups=1, wpb=773.588, bsz=31.987, num_updates=1443, lr=5.46316e-06, gnorm=36.169, clip=0.000, oom=0.000, loss_scale=16.000, wall=2232, train_wall=2105, accuracy=0.87066\n",
      "| epoch 005:    250 / 305 loss=0.498, nll_loss=0.021, ppl=1.01, wps=524, ups=1, wpb=773.303, bsz=31.988, num_updates=1468, lr=5.37544e-06, gnorm=36.936, clip=0.000, oom=0.000, loss_scale=16.000, wall=2269, train_wall=2142, accuracy=0.869971\n",
      "| epoch 005:    275 / 305 loss=0.499, nll_loss=0.021, ppl=1.01, wps=524, ups=1, wpb=772.641, bsz=31.989, num_updates=1493, lr=5.28772e-06, gnorm=36.886, clip=0.000, oom=0.000, loss_scale=16.000, wall=2306, train_wall=2178, accuracy=0.869294\n",
      "| epoch 005:    300 / 305 loss=0.503, nll_loss=0.021, ppl=1.01, wps=523, ups=1, wpb=771.488, bsz=31.990, num_updates=1518, lr=5.2e-06, gnorm=36.777, clip=0.000, oom=0.000, loss_scale=16.000, wall=2343, train_wall=2215, accuracy=0.867276\n",
      "| epoch 005 | loss 0.501 | nll_loss 0.021 | ppl 1.01 | wps 523 | ups 1 | wpb 770.593 | bsz 31.938 | num_updates 1522 | lr 5.18596e-06 | gnorm 36.673 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 2349 | train_wall 2220 | accuracy 0.867673\n",
      "| epoch 005 | valid on 'valid' subset | loss 1.060 | nll_loss 0.044 | ppl 1.03 | num_updates 1522 | best_accuracy 0.779575 | accuracy 0.768137\n",
      "| epoch 006:     25 / 305 loss=0.337, nll_loss=0.014, ppl=1.01, wps=524, ups=1, wpb=775.962, bsz=32.000, num_updates=1548, lr=5.09474e-06, gnorm=28.036, clip=0.000, oom=0.000, loss_scale=16.000, wall=2404, train_wall=2258, accuracy=0.917067\n",
      "| epoch 006:     50 / 305 loss=0.341, nll_loss=0.014, ppl=1.01, wps=522, ups=1, wpb=773.333, bsz=32.000, num_updates=1573, lr=5.00702e-06, gnorm=29.563, clip=0.000, oom=0.000, loss_scale=16.000, wall=2441, train_wall=2294, accuracy=0.912377\n",
      "| epoch 006:     75 / 305 loss=0.366, nll_loss=0.015, ppl=1.01, wps=522, ups=1, wpb=772.750, bsz=32.000, num_updates=1598, lr=4.9193e-06, gnorm=30.312, clip=0.000, oom=0.000, loss_scale=16.000, wall=2478, train_wall=2330, accuracy=0.901727\n",
      "| epoch 006:    100 / 305 loss=0.361, nll_loss=0.015, ppl=1.01, wps=522, ups=1, wpb=771.178, bsz=32.000, num_updates=1623, lr=4.83158e-06, gnorm=29.965, clip=0.000, oom=0.000, loss_scale=16.000, wall=2515, train_wall=2367, accuracy=0.903156\n",
      "| epoch 006:    125 / 305 loss=0.374, nll_loss=0.016, ppl=1.01, wps=521, ups=1, wpb=770.659, bsz=32.000, num_updates=1648, lr=4.74386e-06, gnorm=31.101, clip=0.000, oom=0.000, loss_scale=16.000, wall=2552, train_wall=2403, accuracy=0.900794\n",
      "| epoch 006:    150 / 305 loss=0.377, nll_loss=0.016, ppl=1.01, wps=521, ups=1, wpb=769.748, bsz=31.980, num_updates=1673, lr=4.65614e-06, gnorm=31.000, clip=0.000, oom=0.000, loss_scale=16.000, wall=2589, train_wall=2439, accuracy=0.898116\n",
      "| epoch 006:    175 / 305 loss=0.369, nll_loss=0.015, ppl=1.01, wps=521, ups=1, wpb=771.665, bsz=31.983, num_updates=1698, lr=4.56842e-06, gnorm=30.836, clip=0.000, oom=0.000, loss_scale=16.000, wall=2626, train_wall=2476, accuracy=0.899272\n",
      "| epoch 006:    200 / 305 loss=0.366, nll_loss=0.015, ppl=1.01, wps=521, ups=1, wpb=770.254, bsz=31.985, num_updates=1723, lr=4.4807e-06, gnorm=31.350, clip=0.000, oom=0.000, loss_scale=16.000, wall=2663, train_wall=2512, accuracy=0.899051\n",
      "| epoch 006:    225 / 305 loss=0.360, nll_loss=0.015, ppl=1.01, wps=521, ups=1, wpb=770.363, bsz=31.987, num_updates=1748, lr=4.39298e-06, gnorm=31.596, clip=0.000, oom=0.000, loss_scale=16.000, wall=2700, train_wall=2548, accuracy=0.901923\n",
      "| epoch 006:    250 / 305 loss=0.364, nll_loss=0.015, ppl=1.01, wps=522, ups=1, wpb=771.705, bsz=31.988, num_updates=1773, lr=4.30526e-06, gnorm=31.979, clip=0.000, oom=0.000, loss_scale=16.000, wall=2737, train_wall=2585, accuracy=0.900361\n",
      "| epoch 006:    275 / 305 loss=0.368, nll_loss=0.015, ppl=1.01, wps=522, ups=1, wpb=772.120, bsz=31.989, num_updates=1798, lr=4.21754e-06, gnorm=31.946, clip=0.000, oom=0.000, loss_scale=16.000, wall=2774, train_wall=2621, accuracy=0.900215\n",
      "| epoch 006:    300 / 305 loss=0.369, nll_loss=0.015, ppl=1.01, wps=522, ups=1, wpb=771.857, bsz=31.990, num_updates=1823, lr=4.12982e-06, gnorm=31.723, clip=0.000, oom=0.000, loss_scale=16.000, wall=2811, train_wall=2658, accuracy=0.900197\n",
      "| epoch 006 | loss 0.369 | nll_loss 0.015 | ppl 1.01 | wps 522 | ups 1 | wpb 770.593 | bsz 31.938 | num_updates 1827 | lr 4.11579e-06 | gnorm 31.748 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 2816 | train_wall 2663 | accuracy 0.900421\n",
      "| epoch 006 | valid on 'valid' subset | loss 1.113 | nll_loss 0.046 | ppl 1.03 | num_updates 1827 | best_accuracy 0.779575 | accuracy 0.765686\n",
      "| epoch 007:     25 / 305 loss=0.294, nll_loss=0.012, ppl=1.01, wps=524, ups=1, wpb=783.577, bsz=32.000, num_updates=1853, lr=4.02456e-06, gnorm=26.401, clip=0.000, oom=0.000, loss_scale=16.000, wall=2872, train_wall=2701, accuracy=0.919471\n",
      "| epoch 007:     50 / 305 loss=0.296, nll_loss=0.012, ppl=1.01, wps=523, ups=1, wpb=776.686, bsz=32.000, num_updates=1878, lr=3.93684e-06, gnorm=27.284, clip=0.000, oom=0.000, loss_scale=16.000, wall=2909, train_wall=2737, accuracy=0.924632\n",
      "| epoch 007:     75 / 305 loss=0.279, nll_loss=0.011, ppl=1.01, wps=523, ups=1, wpb=778.500, bsz=32.000, num_updates=1903, lr=3.84912e-06, gnorm=28.362, clip=0.000, oom=0.000, loss_scale=16.000, wall=2946, train_wall=2774, accuracy=0.929276\n",
      "| epoch 007:    100 / 305 loss=0.289, nll_loss=0.012, ppl=1.01, wps=523, ups=1, wpb=775.436, bsz=32.000, num_updates=1928, lr=3.7614e-06, gnorm=29.335, clip=0.000, oom=0.000, loss_scale=16.000, wall=2983, train_wall=2810, accuracy=0.92729\n",
      "| epoch 007:    125 / 305 loss=0.282, nll_loss=0.012, ppl=1.01, wps=524, ups=1, wpb=774.643, bsz=32.000, num_updates=1953, lr=3.67368e-06, gnorm=29.005, clip=0.000, oom=0.000, loss_scale=16.000, wall=3020, train_wall=2846, accuracy=0.929067\n",
      "| epoch 007:    150 / 305 loss=0.281, nll_loss=0.012, ppl=1.01, wps=523, ups=1, wpb=774.563, bsz=32.000, num_updates=1978, lr=3.58596e-06, gnorm=29.416, clip=0.000, oom=0.000, loss_scale=16.000, wall=3057, train_wall=2883, accuracy=0.929222\n",
      "| epoch 007:    175 / 305 loss=0.281, nll_loss=0.012, ppl=1.01, wps=523, ups=1, wpb=773.756, bsz=32.000, num_updates=2003, lr=3.49825e-06, gnorm=29.437, clip=0.000, oom=0.000, loss_scale=16.000, wall=3094, train_wall=2919, accuracy=0.928622\n",
      "| epoch 007:    200 / 305 loss=0.284, nll_loss=0.012, ppl=1.01, wps=522, ups=1, wpb=772.323, bsz=32.000, num_updates=2028, lr=3.41053e-06, gnorm=29.451, clip=0.000, oom=0.000, loss_scale=16.000, wall=3130, train_wall=2955, accuracy=0.927861\n",
      "| epoch 007:    225 / 305 loss=0.275, nll_loss=0.011, ppl=1.01, wps=522, ups=1, wpb=772.602, bsz=32.000, num_updates=2053, lr=3.32281e-06, gnorm=29.039, clip=0.000, oom=0.000, loss_scale=16.000, wall=3167, train_wall=2991, accuracy=0.929342\n",
      "| epoch 007:    250 / 305 loss=0.283, nll_loss=0.012, ppl=1.01, wps=522, ups=1, wpb=770.880, bsz=31.988, num_updates=2078, lr=3.23509e-06, gnorm=30.076, clip=0.000, oom=0.000, loss_scale=16.000, wall=3204, train_wall=3027, accuracy=0.928136\n",
      "| epoch 007:    275 / 305 loss=0.286, nll_loss=0.012, ppl=1.01, wps=522, ups=1, wpb=771.822, bsz=31.989, num_updates=2103, lr=3.14737e-06, gnorm=29.988, clip=0.000, oom=0.000, loss_scale=16.000, wall=3241, train_wall=3064, accuracy=0.926606\n",
      "| epoch 007:    300 / 305 loss=0.285, nll_loss=0.012, ppl=1.01, wps=522, ups=1, wpb=771.850, bsz=31.990, num_updates=2128, lr=3.05965e-06, gnorm=30.055, clip=0.000, oom=0.000, loss_scale=16.000, wall=3278, train_wall=3100, accuracy=0.926161\n",
      "| epoch 007 | loss 0.285 | nll_loss 0.012 | ppl 1.01 | wps 522 | ups 1 | wpb 770.593 | bsz 31.938 | num_updates 2132 | lr 3.04561e-06 | gnorm 30.177 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 3283 | train_wall 3105 | accuracy 0.926188\n",
      "| epoch 007 | valid on 'valid' subset | loss 1.254 | nll_loss 0.052 | ppl 1.04 | num_updates 2132 | best_accuracy 0.779575 | accuracy 0.761928\n",
      "| epoch 008:     25 / 305 loss=0.235, nll_loss=0.010, ppl=1.01, wps=518, ups=1, wpb=766.962, bsz=32.000, num_updates=2158, lr=2.95439e-06, gnorm=26.465, clip=0.000, oom=0.000, loss_scale=16.000, wall=3339, train_wall=3143, accuracy=0.932692\n",
      "| epoch 008:     50 / 305 loss=0.235, nll_loss=0.010, ppl=1.01, wps=519, ups=1, wpb=768.882, bsz=32.000, num_updates=2183, lr=2.86667e-06, gnorm=28.099, clip=0.000, oom=0.000, loss_scale=16.000, wall=3376, train_wall=3180, accuracy=0.936275\n",
      "| epoch 008:     75 / 305 loss=0.235, nll_loss=0.010, ppl=1.01, wps=521, ups=1, wpb=768.671, bsz=32.000, num_updates=2208, lr=2.77895e-06, gnorm=28.347, clip=0.000, oom=0.000, loss_scale=16.000, wall=3413, train_wall=3216, accuracy=0.938322\n",
      "| epoch 008:    100 / 305 loss=0.239, nll_loss=0.010, ppl=1.01, wps=524, ups=1, wpb=772.158, bsz=32.000, num_updates=2233, lr=2.69123e-06, gnorm=29.692, clip=0.000, oom=0.000, loss_scale=16.000, wall=3449, train_wall=3252, accuracy=0.939047\n",
      "| epoch 008:    125 / 305 loss=0.227, nll_loss=0.009, ppl=1.01, wps=523, ups=1, wpb=770.984, bsz=32.000, num_updates=2258, lr=2.60351e-06, gnorm=28.174, clip=0.000, oom=0.000, loss_scale=16.000, wall=3486, train_wall=3288, accuracy=0.94246\n",
      "| epoch 008:    150 / 305 loss=0.228, nll_loss=0.009, ppl=1.01, wps=524, ups=1, wpb=770.311, bsz=32.000, num_updates=2283, lr=2.51579e-06, gnorm=29.265, clip=0.000, oom=0.000, loss_scale=16.000, wall=3522, train_wall=3324, accuracy=0.941846\n",
      "| epoch 008:    175 / 305 loss=0.221, nll_loss=0.009, ppl=1.01, wps=524, ups=1, wpb=771.795, bsz=32.000, num_updates=2308, lr=2.42807e-06, gnorm=29.245, clip=0.000, oom=0.000, loss_scale=16.000, wall=3559, train_wall=3360, accuracy=0.94407\n",
      "| epoch 008:    200 / 305 loss=0.219, nll_loss=0.009, ppl=1.01, wps=524, ups=1, wpb=771.841, bsz=32.000, num_updates=2333, lr=2.34035e-06, gnorm=29.314, clip=0.000, oom=0.000, loss_scale=16.000, wall=3596, train_wall=3397, accuracy=0.944807\n",
      "| epoch 008:    225 / 305 loss=0.220, nll_loss=0.009, ppl=1.01, wps=523, ups=1, wpb=770.699, bsz=31.987, num_updates=2358, lr=2.25263e-06, gnorm=29.763, clip=0.000, oom=0.000, loss_scale=16.000, wall=3634, train_wall=3433, accuracy=0.944391\n",
      "| epoch 008:    250 / 305 loss=0.222, nll_loss=0.009, ppl=1.01, wps=524, ups=1, wpb=771.486, bsz=31.988, num_updates=2383, lr=2.16491e-06, gnorm=30.217, clip=0.000, oom=0.000, loss_scale=16.000, wall=3670, train_wall=3469, accuracy=0.943829\n",
      "| epoch 008:    275 / 305 loss=0.223, nll_loss=0.009, ppl=1.01, wps=524, ups=1, wpb=771.569, bsz=31.989, num_updates=2408, lr=2.07719e-06, gnorm=30.277, clip=0.000, oom=0.000, loss_scale=16.000, wall=3707, train_wall=3505, accuracy=0.942915\n",
      "| epoch 008:    300 / 305 loss=0.231, nll_loss=0.010, ppl=1.01, wps=525, ups=1, wpb=771.940, bsz=31.990, num_updates=2433, lr=1.98947e-06, gnorm=31.258, clip=0.000, oom=0.000, loss_scale=16.000, wall=3743, train_wall=3541, accuracy=0.940181\n",
      "| epoch 008 | loss 0.231 | nll_loss 0.010 | ppl 1.01 | wps 525 | ups 1 | wpb 770.593 | bsz 31.938 | num_updates 2437 | lr 1.97544e-06 | gnorm 31.235 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 3748 | train_wall 3546 | accuracy 0.940047\n",
      "| epoch 008 | valid on 'valid' subset | loss 1.296 | nll_loss 0.054 | ppl 1.04 | num_updates 2437 | best_accuracy 0.779575 | accuracy 0.769281\n",
      "| epoch 009:     25 / 305 loss=0.180, nll_loss=0.007, ppl=1.01, wps=527, ups=1, wpb=772.154, bsz=32.000, num_updates=2463, lr=1.88421e-06, gnorm=27.306, clip=0.000, oom=0.000, loss_scale=16.000, wall=3803, train_wall=3583, accuracy=0.951923\n",
      "| epoch 009:     50 / 305 loss=0.187, nll_loss=0.008, ppl=1.01, wps=525, ups=1, wpb=768.863, bsz=32.000, num_updates=2488, lr=1.79649e-06, gnorm=27.836, clip=0.000, oom=0.000, loss_scale=16.000, wall=3839, train_wall=3619, accuracy=0.951593\n",
      "| epoch 009:     75 / 305 loss=0.188, nll_loss=0.008, ppl=1.01, wps=526, ups=1, wpb=770.579, bsz=32.000, num_updates=2513, lr=1.70877e-06, gnorm=27.773, clip=0.000, oom=0.000, loss_scale=16.000, wall=3876, train_wall=3655, accuracy=0.950247\n",
      "| epoch 009:    100 / 305 loss=0.185, nll_loss=0.008, ppl=1.01, wps=529, ups=1, wpb=774.307, bsz=32.000, num_updates=2538, lr=1.62105e-06, gnorm=28.507, clip=0.000, oom=0.000, loss_scale=16.000, wall=3913, train_wall=3691, accuracy=0.952661\n",
      "| epoch 009:    125 / 305 loss=0.193, nll_loss=0.008, ppl=1.01, wps=529, ups=1, wpb=772.952, bsz=31.976, num_updates=2563, lr=1.53333e-06, gnorm=29.068, clip=0.000, oom=0.000, loss_scale=16.000, wall=3949, train_wall=3727, accuracy=0.950608\n",
      "| epoch 009:    150 / 305 loss=0.195, nll_loss=0.008, ppl=1.01, wps=528, ups=1, wpb=773.079, bsz=31.980, num_updates=2588, lr=1.44561e-06, gnorm=32.101, clip=0.000, oom=0.000, loss_scale=16.000, wall=3986, train_wall=3763, accuracy=0.948851\n",
      "| epoch 009:    175 / 305 loss=0.196, nll_loss=0.008, ppl=1.01, wps=529, ups=1, wpb=774.261, bsz=31.983, num_updates=2613, lr=1.35789e-06, gnorm=31.512, clip=0.000, oom=0.000, loss_scale=16.000, wall=4023, train_wall=3800, accuracy=0.947415\n",
      "| epoch 009:    200 / 305 loss=0.194, nll_loss=0.008, ppl=1.01, wps=528, ups=1, wpb=773.478, bsz=31.985, num_updates=2638, lr=1.27018e-06, gnorm=30.808, clip=0.000, oom=0.000, loss_scale=16.000, wall=4059, train_wall=3836, accuracy=0.948981\n",
      "| epoch 009:    225 / 305 loss=0.192, nll_loss=0.008, ppl=1.01, wps=528, ups=1, wpb=773.270, bsz=31.987, num_updates=2663, lr=1.18246e-06, gnorm=30.684, clip=0.000, oom=0.000, loss_scale=16.000, wall=4096, train_wall=3871, accuracy=0.948817\n",
      "| epoch 009:    250 / 305 loss=0.193, nll_loss=0.008, ppl=1.01, wps=528, ups=1, wpb=772.853, bsz=31.988, num_updates=2688, lr=1.09474e-06, gnorm=30.240, clip=0.000, oom=0.000, loss_scale=16.000, wall=4132, train_wall=3907, accuracy=0.947939\n",
      "| epoch 009:    275 / 305 loss=0.195, nll_loss=0.008, ppl=1.01, wps=527, ups=1, wpb=771.775, bsz=31.989, num_updates=2713, lr=1.00702e-06, gnorm=30.350, clip=0.000, oom=0.000, loss_scale=16.000, wall=4169, train_wall=3943, accuracy=0.947786\n",
      "| epoch 009:    300 / 305 loss=0.191, nll_loss=0.008, ppl=1.01, wps=527, ups=1, wpb=772.063, bsz=31.990, num_updates=2738, lr=9.19298e-07, gnorm=29.691, clip=0.000, oom=0.000, loss_scale=16.000, wall=4206, train_wall=3980, accuracy=0.949424\n",
      "| epoch 009 | loss 0.191 | nll_loss 0.008 | ppl 1.01 | wps 527 | ups 1 | wpb 770.593 | bsz 31.938 | num_updates 2742 | lr 9.05263e-07 | gnorm 29.797 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 4211 | train_wall 3985 | accuracy 0.949389\n",
      "| epoch 009 | valid on 'valid' subset | loss 1.385 | nll_loss 0.058 | ppl 1.04 | num_updates 2742 | best_accuracy 0.779575 | accuracy 0.770098\n",
      "| epoch 010:     25 / 305 loss=0.155, nll_loss=0.007, ppl=1, wps=524, ups=1, wpb=762.269, bsz=32.000, num_updates=2768, lr=8.14035e-07, gnorm=26.990, clip=0.000, oom=0.000, loss_scale=16.000, wall=4266, train_wall=4022, accuracy=0.959135\n",
      "| epoch 010:     50 / 305 loss=0.154, nll_loss=0.006, ppl=1, wps=522, ups=1, wpb=769.804, bsz=32.000, num_updates=2793, lr=7.26316e-07, gnorm=25.508, clip=0.000, oom=0.000, loss_scale=16.000, wall=4303, train_wall=4059, accuracy=0.959559\n",
      "| epoch 010:     75 / 305 loss=0.153, nll_loss=0.006, ppl=1, wps=523, ups=1, wpb=770.974, bsz=32.000, num_updates=2818, lr=6.38596e-07, gnorm=25.674, clip=0.000, oom=0.000, loss_scale=16.000, wall=4340, train_wall=4095, accuracy=0.958882\n",
      "| epoch 010:    100 / 305 loss=0.154, nll_loss=0.006, ppl=1, wps=523, ups=1, wpb=772.149, bsz=32.000, num_updates=2843, lr=5.50877e-07, gnorm=26.234, clip=0.000, oom=0.000, loss_scale=16.000, wall=4377, train_wall=4132, accuracy=0.957921\n",
      "| epoch 010:    125 / 305 loss=0.156, nll_loss=0.006, ppl=1, wps=522, ups=1, wpb=771.619, bsz=31.976, num_updates=2868, lr=4.63158e-07, gnorm=26.536, clip=0.000, oom=0.000, loss_scale=16.000, wall=4414, train_wall=4168, accuracy=0.95731\n",
      "| epoch 010:    150 / 305 loss=0.156, nll_loss=0.006, ppl=1, wps=522, ups=1, wpb=771.311, bsz=31.980, num_updates=2893, lr=3.75439e-07, gnorm=27.568, clip=0.000, oom=0.000, loss_scale=16.000, wall=4451, train_wall=4204, accuracy=0.957962\n",
      "| epoch 010:    175 / 305 loss=0.156, nll_loss=0.006, ppl=1, wps=522, ups=1, wpb=771.324, bsz=31.983, num_updates=2918, lr=2.87719e-07, gnorm=29.880, clip=0.000, oom=0.000, loss_scale=16.000, wall=4488, train_wall=4241, accuracy=0.957186\n",
      "| epoch 010:    200 / 305 loss=0.158, nll_loss=0.007, ppl=1, wps=522, ups=1, wpb=771.045, bsz=31.985, num_updates=2943, lr=2e-07, gnorm=29.721, clip=0.000, oom=0.000, loss_scale=16.000, wall=4525, train_wall=4277, accuracy=0.957381\n",
      "| epoch 010:    225 / 305 loss=0.159, nll_loss=0.007, ppl=1, wps=521, ups=1, wpb=769.571, bsz=31.987, num_updates=2968, lr=1.12281e-07, gnorm=30.454, clip=0.000, oom=0.000, loss_scale=16.000, wall=4562, train_wall=4313, accuracy=0.957255\n",
      "| epoch 010:    250 / 305 loss=0.160, nll_loss=0.007, ppl=1, wps=521, ups=1, wpb=769.534, bsz=31.988, num_updates=2993, lr=2.45614e-08, gnorm=30.700, clip=0.000, oom=0.000, loss_scale=16.000, wall=4599, train_wall=4349, accuracy=0.957529\n",
      "| epoch 010 | loss 0.159 | nll_loss 0.007 | ppl 1 | wps 522 | ups 1 | wpb 769.748 | bsz 31.988 | num_updates 3000 | lr 0 | gnorm 30.412 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 4609 | train_wall 4359 | accuracy 0.957955\n",
      "| epoch 010 | valid on 'valid' subset | loss 1.446 | nll_loss 0.060 | ppl 1.04 | num_updates 3000 | best_accuracy 0.779575 | accuracy 0.768464\n",
      "| done training in 4617.5 seconds\n"
     ]
    }
   ],
   "source": [
    "# Finetune\n",
    "!bash finetune.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/CommonsenseQA/fairseq\n",
      "/home/jupyter/CommonsenseQA/fairseq\n"
     ]
    }
   ],
   "source": [
    "%cd /home/jupyter/CommonsenseQA/fairseq\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/CommonsenseQA/fairseq/examples/roberta/commonsense_qa\n",
      "['/home/jupyter', '/opt/anaconda3/lib/python37.zip', '/opt/anaconda3/lib/python3.7', '/opt/anaconda3/lib/python3.7/lib-dynload', '', '/opt/anaconda3/lib/python3.7/site-packages', '/opt/anaconda3/lib/python3.7/site-packages/IPython/extensions', '/home/jupyter/.ipython']\n"
     ]
    }
   ],
   "source": [
    "# Try to resolve import path issues\n",
    "\n",
    "%cd /home/jupyter/CommonsenseQA/fairseq/examples/roberta/commonsense_qa\n",
    "import sys\n",
    "# sys.path.insert(0, '/home/jupyter/CommonsenseQA/fairseq')\n",
    "# sys.path.insert(0, '/home/jupyter/CommonsenseQA/fairseq/examples/roberta/commonsense_qa')\n",
    "print(sys.path)\n",
    "# import examples\n",
    "# from examples.roberta import commonsense_qa\n",
    "import commonsense_qa_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/CommonsenseQA/fairseq/examples/roberta/commonsense_qa\n",
      "loading archive file /home/jupyter/CommonsenseQA/fairseq/checkpoints\n",
      "loading archive file /home/jupyter/CommonsenseQA/data/CommonsenseQA\n",
      "| dictionary: 50265 types\n",
      "0\n",
      "1\n",
      "3\n",
      "4\n",
      "5\n",
      "Accuracy: 956/1221 = 0.782964782964783\n"
     ]
    }
   ],
   "source": [
    "%cd /home/jupyter/CommonsenseQA/fairseq/examples/roberta/commonsense_qa\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from fairseq.models.roberta import RobertaModel\n",
    "# from examples.roberta import commonsense_qa  # load the Commonsense QA task\n",
    "import commonsense_qa_task  # load the Commonsense QA task\n",
    "\n",
    "base_dir = '/home/jupyter/CommonsenseQA'\n",
    "roberta = RobertaModel.from_pretrained(base_dir + '/fairseq/checkpoints', 'checkpoint_best.pt', base_dir + '/data/CommonsenseQA')\n",
    "print(0)\n",
    "roberta.eval()  # disable dropout\n",
    "print(1)\n",
    "roberta.cuda()  # use the GPU (optional)\n",
    "nsamples, ncorrect = 0, 0\n",
    "wrong = []\n",
    "with open(base_dir + '/data/CommonsenseQA/valid.jsonl') as h:\n",
    "    print(3)\n",
    "    for line in h:\n",
    "        example = json.loads(line)\n",
    "        scores = []\n",
    "        for choice in example['question']['choices']:\n",
    "            input = roberta.encode(\n",
    "                'Q: ' + example['question']['stem'],\n",
    "                'A: ' + choice['text'],\n",
    "                no_separator=True\n",
    "            )\n",
    "            score = roberta.predict('sentence_classification_head', input, return_logits=True)\n",
    "            scores.append(score)\n",
    "#             print(choice['label'], score.data.item())\n",
    "\n",
    "        pred = torch.cat(scores).argmax()\n",
    "#         print('pred: ', chr(ord('A') + pred), 'correct: ', example['answerKey'])\n",
    "        answer = ord(example['answerKey']) - ord('A')\n",
    "        nsamples += 1\n",
    "        if pred == answer:\n",
    "            ncorrect += 1\n",
    "        else:\n",
    "            example['predicted'] = chr(ord('A') + pred)\n",
    "            example['scores'] = {chr(ord('A') + i): s.data.item() for (i, s) in enumerate(scores)}\n",
    "            wrong.append(json.dumps(example))\n",
    "\n",
    "print(4)\n",
    "# Write a file with JSON lines for wrong predictions\n",
    "with open(base_dir + '/wrong_preds.jsonl', 'w') as f:\n",
    "    f.write('\\n'.join(wrong))\n",
    "\n",
    "print(5)\n",
    "print(f'Accuracy: {ncorrect}/{nsamples} = {ncorrect / float(nsamples)}')\n",
    "# Accuracy from FB AI: 0.7846027846027847"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy: 0.782964782964783"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrong_preds jsonl2tsv\n",
    "import json\n",
    "\n",
    "choice_chars = ['A', 'B', 'C', 'D', 'E']\n",
    "tsvlines = ['id\\tquestion_concept\\tquestion\\tchoiceA\\tscoreA\\tchoiceB\\tscoreB\\tchoiceC\\tscoreC\\tchoiceD\\tscoreD\\tchoiceE\\tscoreE\\tanswer\\tpredicted']\n",
    "\n",
    "base_dir = '/home/jupyter/CommonsenseQA'\n",
    "# base_dir = \"D:\\workspace\\ASU\\Courses\\Spring-2020\\CSE-576-Topics-in-Natural-Language-Processing\\Project-COMMONSENSEQA\\\\NLP_CommonsenseQA\\CommonsenseQA\"\n",
    "with open(base_dir + '/wrong_preds.jsonl') as f:\n",
    "    for line in f:\n",
    "        q = json.loads(line)\n",
    "        l = []\n",
    "        l.append(q['id'])\n",
    "        l.append(q['question']['question_concept'])\n",
    "        l.append(q['question']['stem'])\n",
    "\n",
    "        choices = {}\n",
    "        for c in q['question']['choices']:\n",
    "            choices[c['label']] = f\"{c['text']}\\t{round(q['scores'][c['label']], 4)}\"\n",
    "        # To make sure TSV has choices in the order A,B,C,D,E\n",
    "        for c in choice_chars:\n",
    "            l.append(choices[c])\n",
    "\n",
    "        l.append(q['answerKey'])\n",
    "        l.append(q['predicted'])\n",
    "        tsvlines.append('\\t'.join(l))\n",
    "        # print('\\n'.join(tsvlines))\n",
    "        # break\n",
    "\n",
    "with open(base_dir + '/wrong_preds.tsv', 'w') as f:\n",
    "    f.write('\\n'.join(tsvlines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\python37\\lib\\site-packages (2.2.4)\n",
      "Requirement already satisfied: thinc==7.4.0 in c:\\python37\\lib\\site-packages (from spacy) (7.4.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\python37\\lib\\site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\python37\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\python37\\lib\\site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\python37\\lib\\site-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\python37\\lib\\site-packages (from spacy) (2.22.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\python37\\lib\\site-packages (from spacy) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\python37\\lib\\site-packages (from spacy) (1.17.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\python37\\lib\\site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\python37\\lib\\site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\python37\\lib\\site-packages (from spacy) (4.43.0)\n",
      "Requirement already satisfied: setuptools in c:\\python37\\lib\\site-packages (from spacy) (41.6.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\python37\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.7)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\users\\shatru\\appdata\\roaming\\python\\python37\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (0.23)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\shatru\\appdata\\roaming\\python\\python37\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\shatru\\appdata\\roaming\\python\\python37\\site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (7.2.0)\n",
      "You are using pip version 19.0.3, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n",
      "Collecting en_core_web_lg==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz#egg=en_core_web_lg==2.2.5\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9MB)\n",
      "Requirement already satisfied: spacy>=2.2.2 in c:\\python37\\lib\\site-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.17.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.22.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: thinc==7.4.0 in c:\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.43.0)\n",
      "Requirement already satisfied: setuptools in c:\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (41.6.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.25.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2019.9.11)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\users\\shatru\\appdata\\roaming\\python\\python37\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (0.23)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\shatru\\appdata\\roaming\\python\\python37\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\shatru\\appdata\\roaming\\python\\python37\\site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (7.2.0)\n",
      "Installing collected packages: en-core-web-lg\n",
      "  Running setup.py install for en-core-web-lg: started\n",
      "    Running setup.py install for en-core-web-lg: finished with status 'done'\n",
      "Successfully installed en-core-web-lg-2.2.5\n",
      "✔ Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_lg')\n",
      "You are using pip version 19.0.3, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# analysing proper nouns in the validation dataset\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_lg\n",
    "\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bank\n",
      "library\n",
      "department\n",
      "store\n",
      "mall\n",
      "new\n",
      "york\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type Span is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-394aaa7dc4c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'proper_nouns'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mnew_valid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/CommonsenseQA/data/CommonsenseQA/valid-propn.jsonl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python37\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mindent\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mseparators\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m         default is None and not sort_keys and not kw):\n\u001b[1;32m--> 231\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python37\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[1;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python37\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[1;32mC:\\Python37\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m--> 179\u001b[1;33m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[0;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type Span is not JSON serializable"
     ]
    }
   ],
   "source": [
    "# analysing proper nouns in the validation dataset\n",
    "import json\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# def on_match(matcher, doc, id, matches):\n",
    "#     print('Matched!', matches)\n",
    "\n",
    "pattern = [{'POS': 'PROPN'}]  # look for proper nouns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# matcher.add(\"PropNounsInCQA\", [pattern], on_match=on_match)  # matcher.add expects a list of list\n",
    "matcher.add(\"PropNounsInCQA\", [pattern])  # matcher.add expects a list of list\n",
    "\n",
    "new_valid = []\n",
    "# base_path = \"D:\\workspace\\ASU\\Courses\\Spring-2020\\CSE-576-Topics-in-Natural-Language-Processing\\Project-COMMONSENSEQA\\\\NLP_CommonsenseQA\\CommonsenseQA\"\n",
    "base_path = '/home/jupyter/CommonsenseQA'\n",
    "with open(base_path + '/data/CommonsenseQA/valid.jsonl') as f:\n",
    "    for line in f:\n",
    "        q = json.loads(line)\n",
    "        l = []\n",
    "        l.append(q['question']['question_concept'])\n",
    "        l.append(q['question']['stem'])\n",
    "        l += [c['text'] for c in q['question']['choices']]\n",
    "        doc = nlp(' '.join(l))  # get POS tags for concept + question + choices\n",
    "        matches = matcher(doc)\n",
    "        if matches:\n",
    "            # q['has_propn'] = True\n",
    "            q['proper_nouns'] = []\n",
    "        print(doc[:])\n",
    "        for m in matches:\n",
    "            print(doc[m[1]:m[2]])\n",
    "            q['proper_nouns'].append(doc[m[1]:m[2]])\n",
    "\n",
    "        new_valid.append(json.dumps(q))\n",
    "\n",
    "with open(base_path + '/data/CommonsenseQA/valid-propn.jsonl', 'w') as f:\n",
    "    f.write('\\n'.join(new_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geotext\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/c5/36351193092cb4c1d7002d2a3babe5e72ae377868473933d6f63b41e5454/geotext-0.4.0-py2.py3-none-any.whl (2.0MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0MB 2.7MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: geotext\n",
      "Successfully installed geotext-0.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install geotext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'countries': [],\n",
       " 'cities': [],\n",
       " 'nationalities': [],\n",
       " 'country_mentions': OrderedDict()}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from geotext import GeoText\n",
    "\n",
    "# So geotext doesn't seem to work\n",
    "places = GeoText(\"Arizona is a great state\")\n",
    "places.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions with \"what\" predicted wrong: 167/784\n",
      "Number of questions with \"where\" predicted wrong: 81/357\n",
      "Number of questions with \"not\" predicted wrong: 17/74\n",
      "Number of questions with correct answer being 2nd best: 172/265\n"
     ]
    }
   ],
   "source": [
    "# Find questions with where, what, not\n",
    "import json\n",
    "import re\n",
    "\n",
    "whereq = 0\n",
    "whatq = 0\n",
    "notq = 0\n",
    "# base_path = \"D:\\workspace\\ASU\\Courses\\Spring-2020\\CSE-576-Topics-in-Natural-Language-Processing\\Project-COMMONSENSEQA\\\\NLP_CommonsenseQA\\CommonsenseQA\"\n",
    "base_path = '/home/jupyter/CommonsenseQA'\n",
    "with open(base_path + '/data/CommonsenseQA/valid.jsonl') as f:\n",
    "    for line in f:\n",
    "        example = json.loads(line)\n",
    "        ques = example['question']['stem']\n",
    "        # This is very crude and not really accurate as both words might appear in the question and will be added to both counts\n",
    "        if re.search('where', ques, re.IGNORECASE):\n",
    "            whereq += 1\n",
    "        if re.search('what', ques, re.IGNORECASE):\n",
    "            whatq += 1\n",
    "        if re.search('not', ques, re.IGNORECASE):\n",
    "            notq += 1\n",
    "\n",
    "wherew = 0\n",
    "whatw = 0\n",
    "notw = 0\n",
    "second_best = 0\n",
    "total_wrong = 0\n",
    "with open(base_path + '/wrong_preds.jsonl') as f:\n",
    "    for line in f:\n",
    "        total_wrong += 1\n",
    "        example = json.loads(line)\n",
    "        ques = example['question']['stem']\n",
    "        if re.search('where', ques, re.IGNORECASE):\n",
    "            wherew += 1\n",
    "        if re.search('what', ques, re.IGNORECASE):\n",
    "            whatw += 1\n",
    "        if re.search('not', ques, re.IGNORECASE):\n",
    "            notw += 1\n",
    "        # Questions where 2nd best answer was the correct one\n",
    "        if example['scores'][example['answerKey']] == list(sorted(example['scores'].values()))[-2]:\n",
    "            second_best += 1\n",
    "\n",
    "print(f'Number of questions with \"what\" predicted wrong: {whatw}/{whatq}')\n",
    "print(f'Number of questions with \"where\" predicted wrong: {wherew}/{whereq}')\n",
    "print(f'Number of questions with \"not\" predicted wrong: {notw}/{notq}')\n",
    "print(f'Number of questions with correct answer being 2nd best: {second_best}/{total_wrong}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHASE 2 EXPERIMENTS\n",
    "\n",
    "For phase 2, we tried to add external knowledge bases and extract facts for each quetion + answer option combination to train a model with extra knowledge.\n",
    "\n",
    "We used scripts from [McQueen](https://github.com/ari9dam/McQueen) to ingest KB, IR from them, rerank facts using sentence similarity and finally construct the dataset in the format required by McQueen's MCQ solvers.\n",
    "\n",
    "Following 3 experiments were tried:\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-transformers==1.1.0\n",
      "  Using cached https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: regex in /opt/anaconda3/lib/python3.7/site-packages (from pytorch-transformers==1.1.0) (2019.8.19)\n",
      "Collecting boto3\n",
      "  Using cached https://files.pythonhosted.org/packages/c8/1e/587abcd94e8f6dbd42df730f40eb5f7313b6fd7255f5ef5a0db53d116999/boto3-1.13.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: torch>=1.0.0 in /opt/anaconda3/lib/python3.7/site-packages (from pytorch-transformers==1.1.0) (1.4.0+cu100)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.7/site-packages (from pytorch-transformers==1.1.0) (4.42.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.7/site-packages (from pytorch-transformers==1.1.0) (2.22.0)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/9d/dcaaba6fcee6a5c3b36c465557720f088c29cdb5931bc8b4b2556394b3d0/sentencepiece-0.1.86-cp37-cp37m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 2.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/anaconda3/lib/python3.7/site-packages (from pytorch-transformers==1.1.0) (1.18.1)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Using cached https://files.pythonhosted.org/packages/a3/43/1e939e1fcd87b827fe192d0c9fc25b48c5b3368902bfb913de7754b0dc03/jmespath-0.9.5-py2.py3-none-any.whl\n",
      "Collecting botocore<1.17.0,>=1.16.1\n",
      "  Using cached https://files.pythonhosted.org/packages/46/b8/588f44ac91f280beabd0d5ce192a65f50e32e39ebb2a4193590ccb3afff2/botocore-1.16.1-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Using cached https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from requests->pytorch-transformers==1.1.0) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests->pytorch-transformers==1.1.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests->pytorch-transformers==1.1.0) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests->pytorch-transformers==1.1.0) (1.24.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/anaconda3/lib/python3.7/site-packages (from botocore<1.17.0,>=1.16.1->boto3->pytorch-transformers==1.1.0) (2.8.1)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Using cached https://files.pythonhosted.org/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.17.0,>=1.16.1->boto3->pytorch-transformers==1.1.0) (1.12.0)\n",
      "Installing collected packages: jmespath, docutils, botocore, s3transfer, boto3, sentencepiece, pytorch-transformers\n",
      "  Found existing installation: docutils 0.16\n",
      "    Uninstalling docutils-0.16:\n",
      "      Successfully uninstalled docutils-0.16\n",
      "Successfully installed boto3-1.13.1 botocore-1.16.1 docutils-0.15.2 jmespath-0.9.5 pytorch-transformers-1.1.0 s3transfer-0.3.3 sentencepiece-0.1.86\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-transformers==1.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: McQueen Roberta Concat Solver\n",
    "\n",
    "See `logs/mcqueen-robertalg_concat_2e6_009.log` for the training log.\n",
    "\n",
    "**Accuracy: ~20%**\n",
    "\n",
    "See README for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nohup python -u hf_trainer.py --training_data_path /home/jupyter/CommonsenseQA/data/CommonsenseQA/train_mergeIR.jsonl --validation_data_path /home/jupyter/CommonsenseQA/data/CommonsenseQA/valid_mergeIR.jsonl --num_labels 5  --mcq_model roberta-mcq-concat --bert_model roberta-large --output_dir ./robertalg_concat_2e6_009 --num_train_epochs 5 --train_batch_size 64  --do_eval --do_train --max_seq_length 8 --do_lower_case --gradient_accumulation_steps 1  --learning_rate 2e-6 --weight_decay 0.009  --eval_freq 1000 --warmup_steps 250  --overwrite_output_dir > mcqueen-robertalg_concat_2e6_009.log &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Fairseq Roberta Concat with KBs ARC, Webchild, OpenbookQA, and Atomic\n",
    "\n",
    "See `CommonsenseQA/finetune-arc-web-open-atomic.sh` for the finetuning script and  `logs/finetune-arc-web-open-atomic.log` for training log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/CommonsenseQA\n"
     ]
    }
   ],
   "source": [
    "%cd /home/jupyter/CommonsenseQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing finetune-arc-web-open-atomic.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile finetune-arc-web-open-atomic.sh\n",
    "#!/bin/bash\n",
    "\n",
    "## Write the finetuning part to a bash script file\n",
    "# Modified following from the original script to get it to run on Google AI platform and Colab\n",
    "# - Set MAX_SENTENCES=8\n",
    "# - Added --update-freq 4\n",
    "\n",
    "MAX_UPDATES=3000      # Number of training steps.\n",
    "WARMUP_UPDATES=150    # Linearly increase LR over this many steps.\n",
    "LR=1e-05              # Peak LR for polynomial LR scheduler.\n",
    "MAX_SENTENCES=2      # Batch size.\n",
    "SEED=23                # Random seed.\n",
    "\n",
    "BASEDIR=/home/jupyter\n",
    "# CQA_PATH=/content/CommonsenseQA # For Google Colab\n",
    "CQA_PATH=$BASEDIR/CommonsenseQA # For Kaggle\n",
    "ROBERTA_PATH=${BASEDIR}/roberta.large/model.pt\n",
    "DATA_DIR=${CQA_PATH}/data/CommonsenseQA/arc-web-open-atomic\n",
    "\n",
    "# we use the --user-dir option to load the task from\n",
    "# the examples/roberta/commonsense_qa directory:\n",
    "FAIRSEQ_PATH=${CQA_PATH}/fairseq\n",
    "FAIRSEQ_USER_DIR=${FAIRSEQ_PATH}/examples/roberta/commonsense_qa_with_kb\n",
    "\n",
    "cd $FAIRSEQ_PATH\n",
    "CUDA_VISIBLE_DEVICES=0 fairseq-train --fp16 --ddp-backend=no_c10d \\\n",
    "    $DATA_DIR \\\n",
    "    --update-freq 4 \\\n",
    "    --save-dir ./checkpoints \\\n",
    "    --user-dir $FAIRSEQ_USER_DIR \\\n",
    "    --restore-file $ROBERTA_PATH \\\n",
    "    --reset-optimizer --reset-dataloader --reset-meters \\\n",
    "    --no-epoch-checkpoints --no-last-checkpoints --no-save-optimizer-state \\\n",
    "    --best-checkpoint-metric accuracy --maximize-best-checkpoint-metric \\\n",
    "    --task commonsense_qa_with_kb --init-token 0 --bpe gpt2 \\\n",
    "    --arch roberta_large --max-positions 512 \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --criterion sentence_ranking --num-classes 5 \\\n",
    "    --optimizer adam --adam-betas '(0.9, 0.98)' --adam-eps 1e-06 --clip-norm 0.0 \\\n",
    "    --lr-scheduler polynomial_decay --lr $LR \\\n",
    "    --warmup-updates $WARMUP_UPDATES --total-num-update $MAX_UPDATES \\\n",
    "    --max-sentences $MAX_SENTENCES \\\n",
    "    --max-update $MAX_UPDATES \\\n",
    "    --log-format simple --log-interval 25 \\\n",
    "    --seed $SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/CommonsenseQA/fairseq/examples/roberta/commonsense_qa_with_kb\n",
      "loading archive file /home/jupyter/CommonsenseQA/fairseq/checkpoints\n",
      "loading archive file /home/jupyter/CommonsenseQA/data/CommonsenseQA/arc-web-open-atomic\n",
      "| dictionary: 50265 types\n",
      "3\n",
      "Accuracy: 934/1221 = 0.764946764946765\n"
     ]
    }
   ],
   "source": [
    "%cd /home/jupyter/CommonsenseQA/fairseq/examples/roberta/commonsense_qa_with_kb\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from fairseq.models.roberta import RobertaModel\n",
    "# from examples.roberta import commonsense_qa  # load the Commonsense QA task\n",
    "import commonsense_qa_with_kb_task  # load the Commonsense QA task\n",
    "\n",
    "base_dir = '/home/jupyter/CommonsenseQA'\n",
    "roberta = RobertaModel.from_pretrained(base_dir + '/fairseq/checkpoints', 'checkpoint_best.pt', base_dir + '/data/CommonsenseQA/arc-web-open-atomic')\n",
    "# print(0)\n",
    "roberta.eval()  # disable dropout\n",
    "# print(1)\n",
    "roberta.cuda()  # use the GPU (optional)\n",
    "nsamples, ncorrect = 0, 0\n",
    "wrong = []\n",
    "with open(base_dir + '/data/CommonsenseQA/arc-web-open-atomic/valid.jsonl') as h:\n",
    "    print(3)\n",
    "    for line in h:\n",
    "        example = json.loads(line)\n",
    "        scores = []\n",
    "        for i, choice in enumerate(example['choices']):\n",
    "            input = roberta.encode(\n",
    "                'Q: ' + example['question'],\n",
    "                'A: ' + choice + '. ' + '. '.join(example['premises'][i]),\n",
    "                no_separator=True\n",
    "            )[:512]  # truncate to 512 if necessary\n",
    "            score = roberta.predict('sentence_classification_head', input, return_logits=True)\n",
    "            scores.append(score)\n",
    "#             print(choice['label'], score.data.item())\n",
    "\n",
    "        pred = torch.cat(scores).argmax()\n",
    "#         print('pred: ', chr(ord('A') + pred), 'correct: ', example['answerKey'])\n",
    "        answer = example['gold_label']\n",
    "        nsamples += 1\n",
    "        if pred == answer:\n",
    "            ncorrect += 1\n",
    "        else:\n",
    "            example['predicted'] = chr(ord('A') + pred)\n",
    "            example['scores'] = {chr(ord('A') + i): s.data.item() for (i, s) in enumerate(scores)}\n",
    "            wrong.append(json.dumps(example))\n",
    "\n",
    "# print(4)\n",
    "# Write a file with JSON lines for wrong predictions\n",
    "with open(base_dir + '/wrong_preds.jsonl', 'w') as f:\n",
    "    f.write('\\n'.join(wrong))\n",
    "\n",
    "# print(5)\n",
    "print(f'Accuracy: {ncorrect}/{nsamples} = {ncorrect / float(nsamples)}')\n",
    "# Accuracy from FB AI: 0.7846027846027847"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Fairseq Roberta Concat with KBs ARC, Webchild, and ConceptNet\n",
    "\n",
    "See `CommonsenseQA/finetune-web-arc-cn.sh` for the finetuning script and  `logs/finetune-web-arc-cn.log` for training log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/CommonsenseQA/fairseq/examples/roberta/commonsense_qa_with_kb\n",
      "loading archive file /home/jupyter/CommonsenseQA/fairseq/checkpoints\n",
      "loading archive file /home/jupyter/CommonsenseQA/data/CommonsenseQA/web-arc-cn\n",
      "| dictionary: 50265 types\n",
      "0\n",
      "1\n",
      "3\n",
      "4\n",
      "5\n",
      "Accuracy: 935/1221 = 0.7657657657657657\n"
     ]
    }
   ],
   "source": [
    "%cd /home/jupyter/CommonsenseQA/fairseq/examples/roberta/commonsense_qa_with_kb\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from fairseq.models.roberta import RobertaModel\n",
    "# from examples.roberta import commonsense_qa  # load the Commonsense QA task\n",
    "import commonsense_qa_with_kb_task  # load the Commonsense QA task\n",
    "\n",
    "base_dir = '/home/jupyter/CommonsenseQA'\n",
    "roberta = RobertaModel.from_pretrained(base_dir + '/fairseq/checkpoints', 'checkpoint_best.pt', base_dir + '/data/CommonsenseQA/web-arc-cn')\n",
    "print(0)\n",
    "roberta.eval()  # disable dropout\n",
    "print(1)\n",
    "roberta.cuda()  # use the GPU (optional)\n",
    "nsamples, ncorrect = 0, 0\n",
    "wrong = []\n",
    "with open(base_dir + '/data/CommonsenseQA/web-arc-cn/valid.jsonl') as h:\n",
    "    print(3)\n",
    "    for line in h:\n",
    "        example = json.loads(line)\n",
    "        scores = []\n",
    "        for i, choice in enumerate(example['choices']):\n",
    "            input = roberta.encode(\n",
    "                'Q: ' + example['question'],\n",
    "                'A: ' + choice + '. ' + '. '.join(example['premises'][i]),\n",
    "                no_separator=True\n",
    "            )[:512]\n",
    "            score = roberta.predict('sentence_classification_head', input, return_logits=True)\n",
    "            scores.append(score)\n",
    "#             print(choice['label'], score.data.item())\n",
    "\n",
    "        pred = torch.cat(scores).argmax()\n",
    "#         print('pred: ', chr(ord('A') + pred), 'correct: ', example['answerKey'])\n",
    "        answer = example['gold_label']\n",
    "        nsamples += 1\n",
    "        if pred == answer:\n",
    "            ncorrect += 1\n",
    "        else:\n",
    "            example['predicted'] = chr(ord('A') + pred)\n",
    "            example['scores'] = {chr(ord('A') + i): s.data.item() for (i, s) in enumerate(scores)}\n",
    "            wrong.append(json.dumps(example))\n",
    "\n",
    "print(4)\n",
    "# Write a file with JSON lines for wrong predictions\n",
    "with open(base_dir + '/wrong_preds.jsonl', 'w') as f:\n",
    "    f.write('\\n'.join(wrong))\n",
    "\n",
    "print(5)\n",
    "print(f'Accuracy: {ncorrect}/{nsamples} = {ncorrect / float(nsamples)}')\n",
    "# Accuracy from FB AI: 0.7846027846027847"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}